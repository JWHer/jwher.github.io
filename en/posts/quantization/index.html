<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">정확한 추론을 위한 양자화 기법 | JWHer Tech Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jwher.github.io/en/posts/quantization"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="정확한 추론을 위한 양자화 기법 | JWHer Tech Blog"><meta data-rh="true" name="description" content="양자화를 통한 딥러닝 모델 경량화"><meta data-rh="true" property="og:description" content="양자화를 통한 딥러닝 모델 경량화"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2025-04-15T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jwher"><meta data-rh="true" property="article:tag" content="ml"><link data-rh="true" rel="icon" href="/en/img/logo.svg"><link data-rh="true" rel="canonical" href="https://jwher.github.io/en/posts/quantization"><link data-rh="true" rel="alternate" href="https://jwher.github.io/en/posts/quantization" hreflang="en"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/quantization" hreflang="kr"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/quantization" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/en/posts/rss.xml" title="JWHer Tech Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/posts/atom.xml" title="JWHer Tech Blog Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-XHBVCY40VB","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.e87f1c4a.css">
<link rel="preload" href="/en/assets/js/runtime~main.69d68808.js" as="script">
<link rel="preload" href="/en/assets/js/main.9c3efe19.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">JWHer Tech Blog</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/posts">Posts</a><a class="navbar__item navbar__link" href="/en/categories">Categories</a><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-github"></a><a href="https://www.linkedin.com/in/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-linkedin"></a><a href="https://www.instagram.com/jwher96" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-instagram"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="posts__header_mMDK"><div class="filter_lpre"></div><h2 class="presentation__title">Posts</h2><h6 class="presentation__subtitle">Let thine heart retain my words: Keep my commandments, and live.</h6></div><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/en/posts/quantization">정확한 추론을 위한 양자화 기법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/the-case-for-the-reduced-instruction-set-computer">RISC 컴퓨터를 위한 사례</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/docker-networking">도커 네트워킹</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/llama-open-and-efficient-foundation-language-models">LLaMa 공개된 효율적인 언어 모델</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/euclidean-algorithm">유클리드 호제법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/chatgpt-duality-of-wealth-and-faith">ChatGPT: 부와 신앙의 이중성에 대해</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic2">코딩 기초 트레이닝2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic1">코딩 기초 트레이닝1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/discipleship-training-1">지도자의 부르심</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/the-docker-engine">도커 엔진</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/palm">Pathways를 이용한 언어모델 스케일링</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cudabook-1">대규모 병렬프로세서 프로그래밍(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/attention-is-all-you-need">Attention 다시보기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-4">7가지 동시성 모델(스레드와 락) 거인의 어깨 위에서</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-3">7가지 동시성 모델(스레드와 락) 고유 락 개선하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-2">7가지 동시성 모델(스레드와 락) 상호 배제와 메모리 모델</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-1">7가지 동시성 모델(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/requirement-levels">요구사항에 사용하는 RFC 키워드</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/hidden-technical-debt">머신러닝에 숨은 기술 부채</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/api-design-for-long-jobs">오래걸리는 API 설계</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deep-learning-on-a-data-diet">학습에 중요한 데이터 찾기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ngrx">NGRX 반응형 웹을 위한 상태 관리</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/power-series">다양한 급수</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/build-opencv-with-java">Build OpenCV with Java</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/binomial-theorem">이항정리 - π값을 구하는 법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nvidia-gpu-architectures">Nvidia GPU 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/pytorch-in-m1">Pytorch in M1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/agile">Agile</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post-with-docusaurus">First post with docusaurus</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ensemble-methods">Ensemble Methods</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uncertainty-estimation">Uncertainty Estimation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/Intelligent-Computer-Vision-1">Intelligent Computer Vision 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-architecture">쿠버네티스 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-guide">Kubeflow Guide</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/sagemaker">Sagemaker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-tech-map">Blog Tech Map</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cncf">Cncf</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/envoy">Envoy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/istio">Istio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/dex">Dex</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-configmap">K8S Tip Configmap</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/golang-setup">Golang Setup</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/docker-shared-volume">Docker Shared Volume</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/variable-autoencoder">Variable Autoencoder</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-pv-terminating">K8S Tip Pv Terminating</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/free-wildcard-dns">Free Wildcard Dns</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-harbor">Install Harbor</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-helm">Install Helm</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/github-issue">Github Issue</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-rollback">K8S Tip Rollback</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-expose-service">K8S Tip Expose Service</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uuid">Uuid</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deploying-ml-model-on-kubernetes-nuclio">Deploying Ml Model On Kubernetes Nuclio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-2">Kubeflow Visualization 2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/linux-disk-free">Linux Disk Free</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-docker">Welcome To Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubeflow">Welcome To Kubeflow</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/minio">Minio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-nodeselector">K8S Tip Nodeselector</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/information-theory">Information Theory</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-1">Kubeflow Visualization 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nuclio">Nuclio 개념과 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-usage">자주쓰는 쿠버네티스 명령어</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-essay">나에게 필요한 연재에 대해</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/alphapose">Alphapose 논문 리뷰와 사용</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/update-blog">지킬 블로그 업데이트</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-tar.gz">타르(tar) 파일 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubeflow">쿠브플로우를 설치하는 다양한 방법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/virtualbox-with-no-gui">GUI 없이 버추얼박스 사용하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-docker">나에게 필요한 도커 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubernetes">나에게 필요한 쿠버네티스 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubernetes">쿠버네티스 기본 개념과 필요성</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post">First Post with Jekyll</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_xvU1" itemprop="headline">정확한 추론을 위한 양자화 기법</h1><div class="container_iJTo margin-vert--md"><time datetime="2025-04-15T00:00:00.000Z" itemprop="datePublished">April 15, 2025</time> · <!-- -->24 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_q4o9"><div class="avatar margin-bottom--sm"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/jwher.png" alt="Jeongwon Her"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeongwon Her</span></a></div><small class="avatar__subtitle" itemprop="description">MLOps Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><h2 class="anchor anchorWithStickyNavbar_LWe7" id="신경망-경량화-기법">신경망 경량화 기법<a href="#신경망-경량화-기법" class="hash-link" aria-label="Direct link to 신경망 경량화 기법" title="Direct link to 신경망 경량화 기법">​</a></h2><p>지난 10년간, 다양한 문제들에 대해 <strong>신경망(Neural Network, NN)</strong>의 정확도가 상당히 향상되었습니다. 이러한 정확도 향상은 대부분 매우 과대매개변수화된(over-parameterized) 모델에 의해 이루어졌습니다. 하지만 이러한 대규모 모델은 리소스가 제한된 환경에서는 배포가 어려운 문제를 일으킵니다. 이로 인해 실시간 추론, 낮은 에너지 소비, 높은 정확도를 요구하는 리소스 제약 환경에서 딥러닝의 보편적 적용에는 한계가 존재합니다.</p><blockquote><p>효율적인 실시간 NN을 달성하려면, NN의 설계, 학습, 배포 방식을 다시 생각해야 합니다.</p></blockquote><p>이를 해결하기 위한 다양한 연구가 진행되어 왔으며, 지연(latency), 메모리 사용량, 에너지 소비 측면에서 NN 모델을 더욱 효율적으로 만들면서도 최적의 정확도/일반화 균형을 유지하고자 했습니다.</p><ul><li>효율적인 NN 아키텍처 설게</li><li>NN 아키텍처와 하드웨어의 공동 설계</li><li>프루닝</li><li>Knowledge Distillation</li><li><strong>양자화</strong></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="효율적인-nn-아키텍처-설계">효율적인 NN 아키텍처 설계<a href="#효율적인-nn-아키텍처-설계" class="hash-link" aria-label="Direct link to 효율적인 NN 아키텍처 설계" title="Direct link to 효율적인 NN 아키텍처 설계">​</a></h3><ul><li>마이크로 아키텍처 (커널 종류 예: Depthwise convolution, low-rank factorization)</li><li>매크로 아키텍처 (모듈 종류 예: Residual, Inception)</li></ul><p>초기에는 수작업으로 모듈을 찾았지만 이는 확장성이 떨어집니다.
최근(21년)에는 AutoML 및 <strong>NAS (Neural Architecture Search)</strong>를 통해 자동화된 NN 구조 탐색 노력이 이뤄졌습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="nn-아키텍처와-하드웨어의-공동-설계">NN 아키텍처와 하드웨어의 공동 설계<a href="#nn-아키텍처와-하드웨어의-공동-설계" class="hash-link" aria-label="Direct link to NN 아키텍처와 하드웨어의 공동 설계" title="Direct link to NN 아키텍처와 하드웨어의 공동 설계">​</a></h3><p><em>Co-design</em>
NN 연산의 오버헤드는 하드웨어에 따라 다르므로 특정 하드웨어 플랫폼에 적합한 NN 구조를 공동 설계하는 방법이 있습니다.</p><p>예: 캐시 계층이 있는 하드웨어는 bandwidth-bound 연산에 유리</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="프루닝">프루닝<a href="#프루닝" class="hash-link" aria-label="Direct link to 프루닝" title="Direct link to 프루닝">​</a></h3><p><em>Pruning</em>
중요도가 낮은 레이어를 제거하여 희소한 그래프 생성합니다.</p><ul><li>비구조적(unstructured) 프루닝: 텐서 단위 제거 → 가속이 어려움</li><li>구조적(structured) 프루닝: 필터 단위 제거 → 밀집 연산 유지 가능하지만 정확도 저하 우려</li></ul><p>여전히 고수준 프루닝에서 정확도를 유지하는 것은 미해결 과제입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="knowledge-distillation">Knowledge Distillation<a href="#knowledge-distillation" class="hash-link" aria-label="Direct link to Knowledge Distillation" title="Direct link to Knowledge Distillation">​</a></h3><p>큰 모델(teacher)의 soft 확률 출력을 이용해 작은 모델(student)을 훈련합니다.
하지만 KD만으로는 높은 압축률 달성이 어렵습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="양자화">양자화<a href="#양자화" class="hash-link" aria-label="Direct link to 양자화" title="Direct link to 양자화">​</a></h3><p><em>Quantization</em></p><p>양자화는 훈련 및 추론 모두에서 일관된 성공을 보이는 접근 방식입니다.
과거부터 존재하던 수치 표현 문제와 양자화가, NN의 도입으로 새로운 방법으로 주목받고 있습니다.</p><p>이 글에서는 추론 시 양자화를 이야기 하지만, LLM같은 큰 모델이 등장하며 훈련 시 양자화도 중요하게 되었습니다.
특히 반정밀도(FP16)/혼합정밀도 훈련의 성공으로 AI 가속기의 처리량이 획기적으로 증가했습니다.
하지만 반정밀도 아래로 가는 것은 아직 어렵고 많은 튜닝이 필요합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="양자화의-이점">양자화의 이점<a href="#양자화의-이점" class="hash-link" aria-label="Direct link to 양자화의 이점" title="Direct link to 양자화의 이점">​</a></h2><p><em>Benefits of Lower Precision Data Types</em></p><p>실시간 추론과 에너지 효율이 필수적인 응용 환경에서는 정확도를 유지하면서도 모델의 경량화 및 최적화가 중요해졌습니다. 이러한 흐름 속에서 양자화(Quantization)는 정확도-성능-메모리 사용량 간 균형을 맞출 수 있는 실용적 기법으로 주목받고 있습니다.</p><p>대표적으로 추론에 사용되는 데이터 타입은 다음과 같습니다.</p><ul><li><strong>Precision Types</strong>: FP32, FP16, BF16, INT8</li></ul><p><img loading="lazy" alt="fp32" src="/en/assets/images/fp32-6ac27bec0499a4fb141369c575a50672.png" width="886" height="114" class="img_ev3q">
<em>IEEE 754: Standard for floating-point arithmetic</em></p><p>NVIDIA Turing GPU의 실험에 따르면:</p><table><thead><tr><th>Input Data Type</th><th>Math Throughput</th><th>Bandwidth Reduction</th></tr></thead><tbody><tr><td>FP32</td><td>1x</td><td>1x</td></tr><tr><td>INT8</td><td>16x</td><td>4x</td></tr></tbody></table><p>INT8 연산은 FP32에 비해 메모리 대역폭도 4배 절약되며(32-&gt;8bit), exponent align이 필요한 실수 연산은 최대 16배 빠른 처리량을 제공합니다.</p><p>메모리 대역폭 향상은 Memory bound 작업인 LLM에,
실수 연산 처리량은 arithmetic intensive한 행렬곱(GEMM)과 컨볼루션 연산에 큰 영향을 미칩니다.</p><p>이처럼 양자화는 추론 속도 향상, 모델 경량화, 메모리 절감의 장점을 가집니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-fundamentals">Quantization Fundamentals<a href="#quantization-fundamentals" class="hash-link" aria-label="Direct link to Quantization Fundamentals" title="Direct link to Quantization Fundamentals">​</a></h2><p>실수값을 정수 표현으로 변환하기 위해 다음과 같은 선형 변환 방식이 사용됩니다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><mrow><msup><mn>2</mn><mi>b</mi></msup><mo>−</mo><mn>1</mn></mrow><mrow><mi>α</mi><mo>−</mo><mi>β</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">s = \frac{2^b - 1}{\alpha - \beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.4065em;vertical-align:-0.8804em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5261em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">b</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>z</mi><mo>=</mo><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><mi>β</mi><mo>⋅</mo><mi>s</mi><mo stretchy="false">)</mo><mo>−</mo><msup><mn>2</mn><mrow><mi>b</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">z = -\text{round}(\beta \cdot s) - 2^{b-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8991em"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span></div><p>여기서 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">s</span></span></span></span></span>는 <strong>스케일(scale)</strong>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span></span></span></span></span>는 <strong>제로포인트(zero-point)</strong>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>는 <strong>정수 표현의 비트 수</strong>를 의미합니다.</p><p>예를 들어, 32비트 부동소수점(FP32)을 8비트 정수(INT8)로 변환할 경우:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>s</mi><mo>=</mo><mfrac><mn>255</mn><mrow><mi>α</mi><mo>−</mo><mi>β</mi></mrow></mfrac><mo separator="true">,</mo><mspace width="1em"></mspace><mi>z</mi><mo>=</mo><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><mi>β</mi><mo>⋅</mo><mi>s</mi><mo>−</mo><mn>128</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s = \frac{255}{\alpha - \beta}, \quad z = -\text{round}(\beta \cdot s - 128)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.2019em;vertical-align:-0.8804em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">255</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8804em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:1em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.04398em">z</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">−</span><span class="mord text"><span class="mord">round</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">s</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">128</span><span class="mclose">)</span></span></span></span></span></div><p>이와 같이 계산합니다.</p><p><img loading="lazy" alt="affine quantization" src="/en/assets/images/affine-quantization-90237f5014e90522eb44958c5ff88702.png" width="376" height="250" class="img_ev3q"></p><p>정의역 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mi>β</mi><mo separator="true">,</mo><mi>α</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[\beta, \alpha]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mclose">]</span></span></span></span></span>는 실수값을 정수로 변환할 때 기준이 되는 <strong>클리핑 범위(Clipping Range)</strong>입니다. 이 범위 내의 실수는 정수로 매핑되며, 범위를 벗어난 값은 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span> 또는 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span>로 <strong>절단(clipping)</strong>됩니다.</p><p>양자화 방식은 다음과 같이 구분됩니다:</p><ul><li><strong>Asymmetric Quantization (비대칭 양자화)</strong>: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo mathvariant="normal">≠</mo><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\alpha \neq -\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="inner"><span class="mord"><span class="mrel"></span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>일 때  </li><li><strong>Symmetric Quantization (대칭 양자화)</strong>: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mo>−</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\alpha = -\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>로 설정하여 범위를 0을 중심으로 대칭화한 경우</li></ul><p>양자화의 정밀도는 이 클리핑 범위 <!-- -->[<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span>]<!-- -->의 선택에 크게 의존합니다.<br>
<!-- -->범위가 지나치게 좁으면 <strong>중요한 정보가 클리핑되어 정확도가 급격히 하락</strong>할 수 있고,<br>
<!-- -->반대로 범위가 너무 넓으면 <strong>양자화 해상도(precision)가 떨어져 미세한 차이를 표현하지 못하게 됩니다.</strong></p><p>따라서 적절한 범위를 선택하는 것이 <strong>정확한 추론을 위한 양자화의 핵심 요소</strong>입니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-granularity">Quantization Granularity<a href="#quantization-granularity" class="hash-link" aria-label="Direct link to Quantization Granularity" title="Direct link to Quantization Granularity">​</a></h2><p>양자화는 정적 요소인 <strong>모델의 가중치(Weights)</strong>와 실행 시 동적으로 주어지는 <strong>입력 활성화값(Activations)</strong>에 적용됩니다.</p><p>특히 <strong>가중치에 대한 양자화 파라미터</strong>(예: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>, scale, zero-point 등)는 다음과 같은 수준에서 공유될 수 있습니다:</p><ul><li><strong>Layerwise Quantization</strong>: 하나의 레이어 전체에 대해 동일한 파라미터를 적용합니다.  </li><li><strong>Channelwise Quantization</strong>: 각 채널(예: convolution filter)마다 독립적인 파라미터를 사용합니다.  </li><li><strong>Sub-channelwise Quantization</strong>: 채널 내에서도 여러 파라미터 그룹을 정의하여 더욱 세밀하게 범위를 조정합니다.</li></ul><p><img loading="lazy" alt="granularity" src="/en/assets/images/granularity-b638488a06bfb261bf055e5469ce8a51.png" width="794" height="492" class="img_ev3q"></p><p>특히 CNN에서는 <strong>channelwise quantization이 정확도와 계산 효율 간의 균형</strong> 측면에서 가장 널리 사용되고 있으며, 실험적으로도 안정적인 성능을 보입니다.</p><p>한편, <strong>Activation에 대한 양자화</strong> 역시 다양한 수준에서 설정할 수 있으나, 다음과 같은 이유로 일반적으로 <strong>per-tensor granularity</strong>가 사용됩니다:</p><ul><li>Activation은 <strong>실행 시간 중 입력에 따라 변화</strong>하므로, 레이어 단위나 채널 단위로 동적으로 양자화 범위를 조정하는 데 비용이 큽니다.  </li><li>특히 배치 간 shape이 달라지거나 순서를 고려해야 하는 경우, per-channel 방식은 실시간 추론에서 부적절할 수 있습니다.  </li></ul><p>따라서 대부분의 양자화 프레임워크에서는 activation에 대해 <strong>단일 scale과 zero-point를 공유하는 per-tensor quantization</strong>을 적용하여 <strong>속도와 단순성</strong>을 확보합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-challenges">Quantization Challenges<a href="#quantization-challenges" class="hash-link" aria-label="Direct link to Quantization Challenges" title="Direct link to Quantization Challenges">​</a></h2><p>일부 모델은 <strong>activation의 outlier</strong>나 <strong>극단적인 weight 분포</strong>로 인해 양자화에 매우 민감하게 반응합니다. 이러한 경우, 양자화 적용 시 <strong>정확도 저하</strong>가 발생하기 쉽습니다.</p><p>대표적인 대응 기법으로 다음이 있습니다.</p><ul><li><p><strong>SmoothQuant</strong><br>
<!-- -->실행 시 등장하는 outlier activation을 대응하기 위해, 해당 값을 weight에 정렬시키는 방식입니다. 이를 통해 activation의 분포를 정규화하여 양자화 오차를 줄이고, clipping에 의한 정보 손실을 감소시킬 수 있습니다.</p></li><li><p><strong>BF16 (BFloat16)</strong><br>
<!-- -->FP16보다 더 넓은 <strong>동적 범위(dynamic range)</strong>를 제공함으로써 outlier에 보다 강인하게 작동합니다. 다만 일부 연산에서 정확도 손실이 발생할 수 있으며, 일반적으로는 acceptable한 수준으로 평가됩니다.</p></li></ul><p><img loading="lazy" alt="bf16" src="/en/assets/images/bf16-61f9e86a65db6c0ee1a567f55137e663.png" width="520" height="292" class="img_ev3q"></p><ul><li><strong>FP16 표현 범위</strong>: 약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5.96</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>8</mn></mrow></msup></mrow><annotation encoding="application/x-tex">5.96 \times 10^{-8}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">5.96</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">8</span></span></span></span></span></span></span></span></span></span></span></span></span> ~ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>65504</mn></mrow><annotation encoding="application/x-tex">65504</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6444em"></span><span class="mord">65504</span></span></span></span></span>  </li><li><strong>BF16 표현 범위</strong>: 약 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mrow><mo>−</mo><mn>34</mn></mrow></msup></mrow><annotation encoding="application/x-tex">1 \times 10^{-34}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">34</span></span></span></span></span></span></span></span></span></span></span></span></span> ~ <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3.39</mn><mo>×</mo><mn>1</mn><msup><mn>0</mn><mn>38</mn></msup></mrow><annotation encoding="application/x-tex">3.39 \times 10^{38}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em"></span><span class="mord">3.39</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:0.8141em"></span><span class="mord">1</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">38</span></span></span></span></span></span></span></span></span></span></span></span></span></li></ul><p>BF16은 <strong>정밀도는 낮지만 표현 범위가 넓기 때문에</strong>, 큰 값 혹은 작은 값을 포함하는 모델에서도 양자화 오차를 완화하는 데 유리합니다. 특히 대규모 언어 모델(LLM)이나 transformer 계열 구조에서는 BF16이 널리 활용되고 있습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-methodologies">Quantization Methodologies<a href="#quantization-methodologies" class="hash-link" aria-label="Direct link to Quantization Methodologies" title="Direct link to Quantization Methodologies">​</a></h2><p>양자화는 적용 시점과 방식에 따라 크게 두 가지 접근으로 나뉩니다:</p><ul><li><p><strong>Post-Training Quantization (PTQ)</strong><br>
<!-- -->사전 학습이 완료된 모델에 대해 추가 학습 없이 양자화를 적용하는 방식입니다.<br>
<!-- -->대표적인 입력 데이터를 활용해 <strong>activation의 분포를 보정(calibration)</strong>하지만, <strong>가중치는 변경되지 않습니다.</strong><br>
<!-- -->간단하고 빠르며, 소량의 데이터로도 적용할 수 있습니다.</p></li><li><p><strong>Quantization-Aware Training (QAT)</strong><br>
<!-- -->학습 과정에서 양자화 효과를 <strong>시뮬레이션(faked quantization)</strong>하여, 모델이 양자화 오차를 학습 중에 보정할 수 있도록 하는 방식입니다.<br>
<strong>가중치와 활성화 모두 정수 표현을 모방하며 학습되기 때문에</strong>, PTQ보다 일반적으로 <strong>더 높은 정확도 유지</strong>가 가능합니다.<br>
<!-- -->다만 전체 또는 일부 학습을 다시 수행해야 하므로 <strong>비용이 더 크고, 레이블된 데이터가 필요</strong>하다는 단점이 있습니다.</p></li></ul><p><img loading="lazy" alt="method" src="/en/assets/images/quantization-methods-769b657d47563e058b6cf3079f6ae791.png" width="2974" height="980" class="img_ev3q"></p><p>두 방법은 정확도와 효율성 측면에서 상호 보완적으로 활용될 수 있습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="post-training-quantization-ptq">Post-Training Quantization (PTQ)<a href="#post-training-quantization-ptq" class="hash-link" aria-label="Direct link to Post-Training Quantization (PTQ)" title="Direct link to Post-Training Quantization (PTQ)">​</a></h3><p><strong>Post-Training Quantization</strong>은 추가 학습 없이, 이미 학습된 모델에 양자화를 적용하는 방식입니다.<br>
<!-- -->가중치는 그대로 유지되며, 대표적인 입력 데이터를 활용하여 <strong>activation의 분포를 기반으로 스케일(scale)과 제로포인트(zero-point)를 추정</strong>합니다.</p><p><img loading="lazy" alt="histogram" src="/en/assets/images/histogram-974b4899bbe473c33fdc4524c734006a.png" width="589" height="256" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="calibration-알고리즘">Calibration 알고리즘<a href="#calibration-알고리즘" class="hash-link" aria-label="Direct link to Calibration 알고리즘" title="Direct link to Calibration 알고리즘">​</a></h4><p>양자화에서 <strong>클리핑 범위 <!-- -->[<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal" style="margin-right:0.0037em">α</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.05278em">β</span></span></span></span></span>]</strong>를 결정하기 위한 대표적인 알고리즘은 다음과 같습니다:</p><ul><li><strong>Max</strong>: 활성화의 최대 절댓값을 기준으로 범위를 설정 (단순하지만 outlier에 민감)  </li><li><strong>Entropy</strong>: 양자화 전후의 분포 간 <strong>KL divergence</strong>를 최소화하여 정보 손실을 줄이는 방식  </li><li><strong>Percentile</strong>: 분포의 상위 백분위(예: 99.9%)를 기준으로 하여 <strong>outlier의 영향 최소화</strong></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-and-activation-functions">Quantization and Activation Functions<a href="#quantization-and-activation-functions" class="hash-link" aria-label="Direct link to Quantization and Activation Functions" title="Direct link to Quantization and Activation Functions">​</a></h4><p>양자화 정확도는 <strong>활성화 함수의 출력 분포</strong>에도 크게 영향을 받습니다.<br>
<!-- -->특히 <strong>Swish</strong>, <strong>GELU</strong>와 같은 smooth activation function은 <strong>작고 제한된 음수 영역</strong>을 갖고 있으며, 다음과 같은 특성을 가집니다:</p><ul><li><strong>Swish</strong> 출력 범위: <!-- -->[−0.2785, ∞]<!-- -->  </li><li><strong>GELU</strong> 출력 범위: <!-- -->[−0.1700, ∞]</li></ul><p><img loading="lazy" alt="gelu" src="/en/assets/images/gelu-31f180f08d6caad954e0448e64cb8a93.png" width="728" height="232" class="img_ev3q"></p><p>이러한 범위는 대칭 양자화(symmetric quantization)에서 <strong>작은 음수와 큰 양수를 동시에 표현해야 하므로 정밀도 손실이 발생</strong>합니다.<br>
<!-- -->예를 들어, GELU 출력을 <!-- -->[−50, 50]<!-- -->으로 양자화할 경우, 대부분의 음수 출력은 0으로 반올림되며 손실됩니다.<br>
<!-- -->하지만 이를 <!-- -->[−10, 10]<!-- -->으로 제한하면 <strong>두 개 이상의 음수 구간을 표현할 수 있어 정확도가 향상</strong>됩니다.</p><p>이러한 현상은 BERT Large 모델에서 실험적으로 확인되었습니다.</p><table><thead><tr><th>Calibration 방식</th><th>정확도 (F1)</th></tr></thead><tbody><tr><td>FP32 (baseline)</td><td>91.01</td></tr><tr><td>Max</td><td>85.92</td></tr><tr><td>Entropy</td><td>37.40</td></tr><tr><td>99.9%</td><td>26.18</td></tr><tr><td>99.99%</td><td>89.59</td></tr><tr><td>99.999%</td><td>90.20</td></tr><tr><td>99.9999%</td><td>90.10</td></tr><tr><td><strong>Max + GELU10</strong></td><td><strong>90.66</strong></td></tr></tbody></table><blockquote><p>※ &quot;GELU10&quot;은 GELU activation의 출력을 <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mo>−</mo><mn>10</mn><mo separator="true">,</mo><mn>10</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[-10, 10]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">[</span><span class="mord">−</span><span class="mord">10</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord">10</span><span class="mclose">]</span></span></span></span></span>으로 제한한 경우입니다.</p></blockquote><p>이 결과는 다음을 시사합니다:</p><ul><li>복잡한 entropy 기반 보정보다 <strong>단순한 Max calibration과 적절한 출력 클리핑</strong>이 더 나은 결과를 낼 수 있습니다.</li><li><strong>GELU10 + Max calibration은 QAT에서 얻은 최고 성능(90.67)에 거의 근접</strong>합니다.</li><li>즉, <strong>activation 출력의 클리핑 범위를 정제하는 것만으로도 PTQ의 성능을 크게 개선</strong>할 수 있습니다.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="quantization-aware-training-qat">Quantization-Aware Training (QAT)<a href="#quantization-aware-training-qat" class="hash-link" aria-label="Direct link to Quantization-Aware Training (QAT)" title="Direct link to Quantization-Aware Training (QAT)">​</a></h3><p><img loading="lazy" alt="qat" src="/en/assets/images/fakequant-f1f22ea8f1a830ccdda5d3f82b01857b.png" width="402" height="288" class="img_ev3q"></p><p><strong>Quantization-Aware Training(QAT)</strong>은 학습 중에 양자화 효과를 시뮬레이션하여, 모델이 양자화 오차를 <strong>내재적으로 학습하도록 만드는 방식</strong>입니다.<br>
<!-- -->즉, 학습 과정에서 실수 표현을 유지하되, 순전파 시 양자화된 값을 사용하는 <strong>&quot;Fake Quantization&quot; 연산자</strong>를 삽입하여 양자화가 모델 성능에 미치는 영향을 반영합니다.</p><p>이로 인해 모델은 정수 연산의 부정확성을 보정하는 방향으로 파라미터를 조정하게 되며, <strong>추론 시 양자화된 연산으로도 높은 정확도를 유지할 수 있습니다.</strong></p><p><img loading="lazy" alt="flow" src="/en/assets/images/qat-flow-2ce55ebce83a5a01d83467f88b80a35b.png" width="1134" height="466" class="img_ev3q"></p><p>양자화 연산은 <strong>불연속적이고 미분 불가능</strong>하기 때문에, 일반적인 역전파에서는 gradient 계산이 불가능합니다.<br>
<!-- -->이를 해결하기 위해 <strong>Straight-Through Estimator(STE)</strong> 기법이 사용됩니다:</p><ul><li>순전파: <code>round()</code> 또는 <code>clamp()</code>와 같은 양자화 연산을 적용하여 <strong>양자화 효과를 시뮬레이션</strong>  </li><li>역전파: <code>identity</code> 함수로 간주하여 <strong>양자화 연산의 gradient를 무시하고 실수값의 gradient를 그대로 전달</strong></li></ul><p>이 방식은 실제 TensorFlow Lite 및 PyTorch QAT 구현에서도 동일한 방식으로 작동합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="comparison-between-ptq-and-qat">Comparison Between PTQ and QAT<a href="#comparison-between-ptq-and-qat" class="hash-link" aria-label="Direct link to Comparison Between PTQ and QAT" title="Direct link to Comparison Between PTQ and QAT">​</a></h3><p>양자화는 적용 시점과 방식에 따라 크게 두 가지 접근 방식으로 구분됩니다: <strong>Post-Training Quantization(PTQ)</strong>과 <strong>Quantization-Aware Training(QAT)</strong>. 이 두 방식은 정확도, 구현 복잡도, 데이터 요구량 측면에서 명확한 차이를 가집니다.</p><table><thead><tr><th>항목</th><th>PTQ</th><th>QAT</th></tr></thead><tbody><tr><td>재학습 여부</td><td>필요 없음</td><td>필요함</td></tr><tr><td>정밀도 유지</td><td>일부 모델에서 큰 정확도 저하 발생 가능</td><td>대부분의 경우 높은 정확도 유지</td></tr><tr><td>구현 난이도</td><td>간단, 빠름</td><td>복잡, 학습 파이프라인 수정 필요</td></tr><tr><td>데이터 요구</td><td>소량의 unlabeled 데이터로 가능</td><td>레이블된 학습 데이터 필요</td></tr><tr><td>적용 시점</td><td>모델 학습 후 적용</td><td>모델 학습 중 적용</td></tr></tbody></table><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ptq의-장점과-한계">PTQ의 장점과 한계<a href="#ptq의-장점과-한계" class="hash-link" aria-label="Direct link to PTQ의 장점과 한계" title="Direct link to PTQ의 장점과 한계">​</a></h4><ul><li><p><strong>장점</strong>:  </p><ul><li>이미 학습된 모델에 대해 <strong>빠르고 간단하게 적용</strong> 가능  </li><li>소량의 대표 입력 데이터만 있으면 적용 가능 (레이블 불필요)  </li><li>하드웨어 테스트 및 배포 전에 빠른 프로토타이핑 가능  </li></ul></li><li><p><strong>한계</strong>:  </p><ul><li>일부 구조 (예: BERT, EfficientNet, GeLU/Switch activation 포함 모델 등)에서는 <strong>정확도 손실이 크며</strong>,<br>이는 activation 분포가 양자화에 적합하지 않거나 weight range가 불균형할 때 발생함</li></ul></li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="qat의-장점과-한계">QAT의 장점과 한계<a href="#qat의-장점과-한계" class="hash-link" aria-label="Direct link to QAT의 장점과 한계" title="Direct link to QAT의 장점과 한계">​</a></h4><ul><li><p><strong>장점</strong>:  </p><ul><li>학습 중 양자화 효과를 시뮬레이션하여, 모델이 이를 <strong>내재적으로 보정</strong>할 수 있음  </li><li>대부분의 모델에서 <strong>FP32에 가까운 정확도</strong>를 유지할 수 있음  </li><li>최근 연구들에서는 QAT가 <strong>fine-tuning 5~10 epoch만으로도 PTQ보다 우수한 결과</strong>를 달성함 [<!-- -->[30]<!-- -->]</li></ul></li><li><p><strong>한계</strong>:  </p><ul><li>기존 학습 파이프라인에 fake quantization 연산자 삽입 등 <strong>추가 구현 필요</strong>  </li><li><strong>레이블된 데이터가 필요</strong>하고, 적어도 일부 학습을 다시 수행해야 함  </li><li>Hyperparameter tuning 없이 그대로 QAT를 적용하면 <strong>오히려 정확도가 하락</strong>할 수도 있음</li></ul></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="recommended-quantization-workflow">Recommended Quantization Workflow<a href="#recommended-quantization-workflow" class="hash-link" aria-label="Direct link to Recommended Quantization Workflow" title="Direct link to Recommended Quantization Workflow">​</a></h2><p>양자화 기법은 <strong>정확도</strong>, <strong>추론 속도</strong>, <strong>개발 비용</strong> 간의 균형을 고려해야 하는 실무 환경에서, 다음과 같은 단계적 전략으로 적용하는 것이 효과적입니다.</p><ul><li><strong>PTQ</strong>는 효율적이며 대부분의 모델에 빠르게 적용 가능합니다.  </li><li>반면, <strong>QAT</strong>는 정확도에 민감한 모델에서 성능을 보장하기 위해 활용됩니다.</li></ul><p>일반적으로 다음과 같은 <strong>단계적 양자화 워크플로우</strong>를 따릅니다:</p><blockquote><p><strong>PTQ로 baseline 구성 → 민감 레이어 분석 및 부분 양자화 → 성능 부족 시 QAT 적용</strong></p></blockquote><p>이러한 방식은 정확도를 유지하면서도 양자화의 효율을 최대한 활용할 수 있는 실용적인 전략입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="단계별-양자화-절차">단계별 양자화 절차<a href="#단계별-양자화-절차" class="hash-link" aria-label="Direct link to 단계별 양자화 절차" title="Direct link to 단계별 양자화 절차">​</a></h3><ol><li><p><strong>Post-Training Quantization (PTQ)</strong>  </p><ul><li>연산량이 큰 레이어 전체에 양자화를 적용하여 baseline 모델 구성  </li><li>Calibration은 다양한 방법(Max, Entropy, 99.99%, 99.999% percentile 등)을 시도하여 최적값 탐색  </li><li>실험적으로 <strong>activation 분포 clipping 범위</strong>에 따라 성능 차이가 크므로, 다양한 분포 기반 범위 조정이 중요</li></ul></li><li><p><strong>Partial Quantization</strong>  </p><ul><li>민감도 분석(sensitivity analysis)을 통해 <strong>정확도에 큰 영향을 미치는 레이어</strong>를 식별  </li><li>해당 레이어는 FP32 혹은 FP16으로 유지하고, 나머지는 INT8로 양자화하여 성능과 정확도의 절충점을 확보</li></ul></li><li><p><strong>Quantization-Aware Training (QAT)</strong>  </p><ul><li>QAT는 전체 재학습이 아닌 <strong>일부 미세조정(fine-tuning)</strong>으로도 효과를 볼 수 있습니다.  </li><li>일반적으로 기존 학습 스케줄의 <strong>10% 이내 범위</strong>에서 fine-tuning을 수행하며,  </li><li>학습률은 초기의 <strong>1% 수준에서 시작</strong>하여 <strong>annealing 방식</strong>으로 점차 감소시키는 것이 권장</li></ul></li></ol><p>이러한 단계적 접근은 빠른 프로토타이핑, 정밀한 튜닝, 정확도 보존을 모두 충족시킬 수 있어, <strong>대부분의 모델에 안정적이고 효과적인 양자화 전략</strong>으로 자리잡고 있습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">​</a></h2><ul><li>양자화의 효과는 <strong>네트워크 아키텍처의 구조와 특성</strong>에 따라 크게 달라집니다.  </li><li><strong>PTQ와 QAT는 각각의 정확도 요구 수준과 시스템 제약 조건</strong>에 따라 적절히 선택해야 합니다.  </li><li>특히 실무 환경에서는, 모델과 하드웨어에 따라 다양한 조합이 존재하므로,<br><strong>실험 기반의 튜닝 과정이 정확도 손실 없는 양자화 추론을 달성하는 데 핵심적인 역할</strong>을 합니다.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><ul><li>A Survey of Quantization Methods for Efficient Neural Network Inference  </li><li>Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference  </li><li>Integer Quantization for Deep Learning Inference: Principles and Empirical Evaluation</li></ul></div><a href="https://www.buymeacoffee.com/jwher"><img style="margin:20px 0" src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=jwher&amp;button_colour=40DCA5&amp;font_colour=ffffff&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=FFDD00"></a><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_Wr5y"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/en/posts/tags/ml">ml</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/jwher/jwher.github.io/tree/main/posts/2025-04-15-quantization/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer><div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/en/posts/the-case-for-the-reduced-instruction-set-computer"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">RISC 컴퓨터를 위한 사례</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#신경망-경량화-기법" class="table-of-contents__link toc-highlight">신경망 경량화 기법</a><ul><li><a href="#효율적인-nn-아키텍처-설계" class="table-of-contents__link toc-highlight">효율적인 NN 아키텍처 설계</a></li><li><a href="#nn-아키텍처와-하드웨어의-공동-설계" class="table-of-contents__link toc-highlight">NN 아키텍처와 하드웨어의 공동 설계</a></li><li><a href="#프루닝" class="table-of-contents__link toc-highlight">프루닝</a></li><li><a href="#knowledge-distillation" class="table-of-contents__link toc-highlight">Knowledge Distillation</a></li><li><a href="#양자화" class="table-of-contents__link toc-highlight">양자화</a></li></ul></li><li><a href="#양자화의-이점" class="table-of-contents__link toc-highlight">양자화의 이점</a></li><li><a href="#quantization-fundamentals" class="table-of-contents__link toc-highlight">Quantization Fundamentals</a></li><li><a href="#quantization-granularity" class="table-of-contents__link toc-highlight">Quantization Granularity</a></li><li><a href="#quantization-challenges" class="table-of-contents__link toc-highlight">Quantization Challenges</a></li><li><a href="#quantization-methodologies" class="table-of-contents__link toc-highlight">Quantization Methodologies</a><ul><li><a href="#post-training-quantization-ptq" class="table-of-contents__link toc-highlight">Post-Training Quantization (PTQ)</a></li><li><a href="#quantization-aware-training-qat" class="table-of-contents__link toc-highlight">Quantization-Aware Training (QAT)</a></li><li><a href="#comparison-between-ptq-and-qat" class="table-of-contents__link toc-highlight">Comparison Between PTQ and QAT</a></li></ul></li><li><a href="#recommended-quantization-workflow" class="table-of-contents__link toc-highlight">Recommended Quantization Workflow</a><ul><li><a href="#단계별-양자화-절차" class="table-of-contents__link toc-highlight">단계별 양자화 절차</a></li></ul></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">
        <div class="copyright">
          Copyright © 2025, made by JWHer.<span class="heart-icon"></span>
        </div>
        </div></div></div></footer></div>
<script src="/en/assets/js/runtime~main.69d68808.js"></script>
<script src="/en/assets/js/main.9c3efe19.js"></script>
</body>
</html>
<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">Pathways를 이용한 언어모델 스케일링 | JWHer Tech Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jwher.github.io/en/posts/palm"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Pathways를 이용한 언어모델 스케일링 | JWHer Tech Blog"><meta data-rh="true" name="description" content="PaLM: Scaling Language Modeling with Pathways"><meta data-rh="true" property="og:description" content="PaLM: Scaling Language Modeling with Pathways"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-02-11T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jwher"><meta data-rh="true" property="article:tag" content="ml,paper"><link data-rh="true" rel="icon" href="/en/img/logo.svg"><link data-rh="true" rel="canonical" href="https://jwher.github.io/en/posts/palm"><link data-rh="true" rel="alternate" href="https://jwher.github.io/en/posts/palm" hreflang="en"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/palm" hreflang="kr"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/palm" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/en/posts/rss.xml" title="JWHer Tech Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/posts/atom.xml" title="JWHer Tech Blog Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-XHBVCY40VB","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.03aafd3a.css">
<link rel="preload" href="/en/assets/js/runtime~main.050f862b.js" as="script">
<link rel="preload" href="/en/assets/js/main.30d9351e.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">JWHer Tech Blog</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/posts">Posts</a><a class="navbar__item navbar__link" href="/en/categories">Categories</a><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-github"></a><a href="https://www.linkedin.com/in/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-linkedin"></a><a href="https://www.instagram.com/jwher96" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-instagram"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="posts__header_mMDK"><div class="filter_lpre"></div><h2 class="presentation__title">Posts</h2><h6 class="presentation__subtitle">Let thine heart retain my words: Keep my commandments, and live.</h6></div><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic2">코딩 기초 트레이닝2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic1">코딩 기초 트레이닝1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/discipleship-training-1">지도자의 부르심</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/the-docker-engine">도커 엔진</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/en/posts/palm">Pathways를 이용한 언어모델 스케일링</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cudabook-1">대규모 병렬프로세서 프로그래밍(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/attention-is-all-you-need">Attention 다시보기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-4">7가지 동시성 모델(스레드와 락) 거인의 어깨 위에서</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-3">7가지 동시성 모델(스레드와 락) 고유 락 개선하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-2">7가지 동시성 모델(스레드와 락) 상호 배제와 메모리 모델</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-1">7가지 동시성 모델(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/requirement-levels">요구사항에 사용하는 RFC 키워드</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/hidden-technical-debt">머신러닝에 숨은 기술 부채</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/api-design-for-long-jobs">오래걸리는 API 설계</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deep-learning-on-a-data-diet">학습에 중요한 데이터 찾기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ngrx">NGRX 반응형 웹을 위한 상태 관리</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/power-series">다양한 급수</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/build-opencv-with-java">Build OpenCV with Java</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/binomial-theorem">이항정리 - π값을 구하는 법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nvidia-gpu-architectures">Nvidia GPU 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/pytorch-in-m1">Pytorch in M1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/agile">Agile</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post-with-docusaurus">First post with docusaurus</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ensemble-methods">Ensemble Methods</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uncertainty-estimation">Uncertainty Estimation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/Intelligent-Computer-Vision-1">Intelligent Computer Vision 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-architecture">쿠버네티스 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-guide">Kubeflow Guide</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/sagemaker">Sagemaker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-tech-map">Blog Tech Map</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cncf">Cncf</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/envoy">Envoy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/istio">Istio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/dex">Dex</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-configmap">K8S Tip Configmap</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/golang-setup">Golang Setup</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/docker-shared-volume">Docker Shared Volume</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/variable-autoencoder">Variable Autoencoder</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-pv-terminating">K8S Tip Pv Terminating</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/free-wildcard-dns">Free Wildcard Dns</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-harbor">Install Harbor</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-helm">Install Helm</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/github-issue">Github Issue</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-rollback">K8S Tip Rollback</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-expose-service">K8S Tip Expose Service</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uuid">Uuid</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deploying-ml-model-on-kubernetes-nuclio">Deploying Ml Model On Kubernetes Nuclio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-2">Kubeflow Visualization 2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/linux-disk-free">Linux Disk Free</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-docker">Welcome To Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubeflow">Welcome To Kubeflow</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/minio">Minio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-nodeselector">K8S Tip Nodeselector</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/information-theory">Information Theory</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-1">Kubeflow Visualization 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nuclio">Nuclio 개념과 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-usage">자주쓰는 쿠버네티스 명령어</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-essay">나에게 필요한 연재에 대해</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/alphapose">Alphapose 논문 리뷰와 사용</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/update-blog">지킬 블로그 업데이트</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-tar.gz">타르(tar) 파일 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubeflow">쿠브플로우를 설치하는 다양한 방법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/virtualbox-with-no-gui">GUI 없이 버추얼박스 사용하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-docker">나에게 필요한 도커 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubernetes">나에게 필요한 쿠버네티스 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubernetes">쿠버네티스 기본 개념과 필요성</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post">First Post with Jekyll</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_xvU1" itemprop="headline">Pathways를 이용한 언어모델 스케일링</h1><div class="container_iJTo margin-vert--md"><time datetime="2023-02-11T00:00:00.000Z" itemprop="datePublished">February 11, 2023</time> · <!-- -->21 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_q4o9"><div class="avatar margin-bottom--sm"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/jwher.png" alt="Jeongwon Her"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeongwon Her</span></a></div><small class="avatar__subtitle" itemprop="description">MLOps Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="/en/posts/palm"><img loading="lazy" alt="palm" src="/en/assets/images/palm-886a5652802d433a8b9ffcae67a3abff.jpeg" width="400" height="427" class="img_ev3q"></a><br>
<em>최대한 번역된 단어를 통일하였으나 원문을 보는것을 권장합니다.</em>  </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="요약">요약<a href="#요약" class="hash-link" aria-label="Direct link to 요약" title="Direct link to 요약">​</a></h2><p><em>Abstract</em></p><p>대규모 언어 모델은 few-shot learning을 사용한 다양한 자연어 처리 분야에서 눈에 띄는 성과를 보이고 있습니다.
이는 모델을 task-specific하게 적응하기 위한 예제 수를 수를 크게 줄입니다.
더 나아가 few-shot learning에 대한 영향에 대한 이해를 높이기 위해,
540억개의 매개 변수, densely activated, <a href="/en/posts/attention-is-all-you-need">transformer</a> 언어 모델인 Pathways Language Model(PaLM)을 학습시켰습니다.</p><p>PaLM은 6144 TPU v4칩에서 Pathways를 사용해 학습했습니다. Pathways는 다양한 TPU Pods에서 고효율로 학습하길 가능하게 하는 ML system입니다.
수백개의 <strong>언어 이해와 생성</strong> 벤치마크에서 few-shot learning의 sota를 달성함을 보여줍니다.
많은 작업에서 PaLM은 540B는 고성능을 달성합니다. <strong>multi-step reasoning tasks</strong>에서 sota를 달성하고,
최근의 BIG-bench 벤치마크에서 평균 사람보다 나은 성능을 보여줍니다.
상당수의 BIG-bench tasks는 모댈 규모에 따라 불연속적인 성능 개선을 보여주었고,
이는 가장 큰 모델로 확장함에 따라 급격한 성능 증가를 보였음을 뜻합니다.
PaLM은 다양한 벤치마크에서 증명되는 강한 <strong>multilingual tasks</strong>와 <strong>source code generation</strong>을 갖춥니다.
편견(bias)과 독성(toxicity)에 대한 포괄적인 분석을 제공하고, 모델 규모와 관련하여 훈련 데이터 암기(memorization)을 연구합니다.
최종적으로 LLM에 대한 윤리적 고려 사항과 점진적인 완화 전략에 대해 논의합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="1-소개">1. 소개<a href="#1-소개" class="hash-link" aria-label="Direct link to 1. 소개" title="Direct link to 1. 소개">​</a></h2><p>(WIP 사전연구들)</p><p>(GLaM, Gopher, Chinchilla, Megatron-Turing NLG, LaMDA)</p><p>이 논문에서, scaling line으로 언어 모델링 개선을 계속하고 780억 개의 고품질 텍스트 토큰에, densely activated, autoregressive transformer 540억 개의 매개 변수를 훈련합니다.
이것은 여러 텐서 처리 장치(TPU) v4 파드를 포함해 수천 개의 가속기 칩에 걸쳐 <strong>매우 큰 신경망</strong>을 <strong>매우 효율적</strong>으로 훈련할 수 있는 새로운 ML 시스템인
<a href="#pathways-asynchronous-distributed-dataflow-for-ml">Pathways</a>를 사용해 달성했습니다.
새로운 모델은 Pathways Language Model (PaLM)이라고 부르며 수백 개의 자연어, 코드 및 수학적 추론 작업에서 sota few-shot 결과를 달성합니다.</p><p>이 작업의 요점은 다음과 같습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="효율적인-스케일링">효율적인 스케일링<a href="#효율적인-스케일링" class="hash-link" aria-label="Direct link to 효율적인 스케일링" title="Direct link to 효율적인 스케일링">​</a></h3><p><em>Efficient scaling</em></p><p>하나의 모델을 학습하는데 천에서 만개의 고효율 가속기를 사용할 수 있게 하는 새로운 ML system
<a href="#pathways-asynchronous-distributed-dataflow-for-ml">Pathways</a>의 첫번째 대규모 사용을 보입니다.
Pathways를 사용하면서 이전에는 도달할 수 없었던 효율성 수준에서 6144TPU v4 칩에 대한 540B 매개 변수 언어 모델을 훈련했습니다.
대부분 이전의 대형 언어 모델은 단일 TPU 시스템(GLaM)이나 파이프라인 병렬 처리(GPipe)를 사용해
GPU 클러스터(-)나 여러 TPU v3 파드(Gopher)를 확장해 최대 4096 TPU v3 chips을 확장했습니다.
섹션 4에서 우리는 모델 FLOPS를 활용해 PaLM 540B를 6144 칩 두개의 TPU v4 Pods에서
매우 높은 46.2%라는 모델 FLOPs utilization을 달성하였고(이론적인 최고 수치에 비교한 수치)
57.8%의 하드웨어 FLOPs utilization을 보였습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="스케일링을-통한-지속적인-향상">스케일링을 통한 지속적인 향상<a href="#스케일링을-통한-지속적인-향상" class="hash-link" aria-label="Direct link to 스케일링을 통한 지속적인 향상" title="Direct link to 스케일링을 통한 지속적인 향상">​</a></h3><p><em>Continued improvements from scaling</em></p><p>섹션 6에서 다양한 주요 벤치마크를 수행하고, 상당한 차이를 보였습니다.
이것은 대규모 LM의 스케일링 개선이 정체되거나 포화점에 도달하지 않았다는 것을 보여줍니다.
표 4에서 가장 널리 평가된 29개의 영어 이해 벤치마크에서 28개의 sota를 달성했음을 보여줍니다.
이전에 수행된 다음 항목과 비교했습니다(GLaM, GPT-3, Megaron-Turing NLG, Gopher, Chinchilla, LaMDA)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="획기적인-능력">획기적인 능력<a href="#획기적인-능력" class="hash-link" aria-label="Direct link to 획기적인 능력" title="Direct link to 획기적인 능력">​</a></h3><p><em>Breakthrough capabilities</em></p><p>많은 어려운 작업에서 언어 이해와 생성에서 획기적인 능력을 보여줍니다.
특히, 섹션 <a href="#63-reasoning">6.3</a>은 정답을 도출하기 위해 multi-step 수학적 또는 상식적인 추론이 필요한 추론(reasoning) 작업 모음에 대한 평가를 제시합니다.
이전의 sota 결과는 좋은(strong) 결과를 얻기 위해 task-specific한 finetuning, domain-specific 아키텍처 및 task-specific verifiers 조합을 사용합니다.
이 작업에서, 우리는 모델 스케일링이 <a href="#chain-of-thought-prompting-elicits-reasoning-in-large-language-models">chain-of-thought prompting</a>과 결합될 때,
간단한 few-shot 평가가 다양한 추론 작업에서 finetuned sota를 능가하거나 같음을 보여줍니다.
섹션 <a href="#62-big-bench">6.2</a>에서 최근에 출시된 150개 이상의 새로운 언어 이해 및 생성 작업 제품군인
<a href="#beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models">BIG-bench</a>의 획기적인 성능을 강조합니다.
BIG-bench의 대부분은 사람도 올바르게 대답하기가 매우 어렵습니다.
그림 1과 섹션 <a href="#9-%EC%84%A4%EB%AA%85-%EC%82%B4%ED%8E%B4%EB%B3%B4%EA%B8%B0exploring-explanations">9</a>에서, 복잡한 추론 체인을 명시적으로 해석하고 설명하는 PaLM의 탐색 능력을 보여줍니다.</p><blockquote><p><img loading="lazy" alt="그림 1" src="/en/assets/images/figure1-88da2dc765f49fed86f0e52a9845c361.png" width="2322" height="650" class="img_ev3q"><br>
<em>그림1</em>  PaLM이 <em>chain-of-thought prompting</em>을 통해 탐색하는 능력을 보여줍니다.
모든 예제는 PaLM 540B에서 greedy(1-best) decoding을 통해 생성되었습니다. 프롬프트는 섹션 9에서 더 많은 예시를 보여줍니다.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="불연속적인-성능-개선">불연속적인 성능 개선<a href="#불연속적인-성능-개선" class="hash-link" aria-label="Direct link to 불연속적인 성능 개선" title="Direct link to 불연속적인 성능 개선">​</a></h3><p><em>Discontinuous improvements</em></p><p>스케일링 동작을 더 잘 이해하기 위해, 8B, 62B, 540B 세 가지 다른 파라미터 크기(scales)의 결과를 제시합니다.
일반적으로, 62B에서 540B로의 스케일링은 8B에서 62B로의 스케일링과 유사한 성능을 보이며,
이는 신경망 스케일링에서 종종 관찰되는 멱법칙(power law)인 경험 규칙(rule of thumb)과 일치합니다.
(Scaling laws for neural language models. Kaplan et al., 2020).
그러나, 특정 작업의 경우, 우리는 62B에서 540B로의 스케일링이 8B에서 62B로의 스케일링에 비해 정확도가 급격하게 증가하는 불연속적인 개선을 관찰합니다.
그러한 행동은 섹션 <a href="#62-big-bench">6.2</a> BIG-bench 작업 약 25%에서 관찰됩니다.
이는 모델이 충분한 규모를 달성할 때 대규모 LM의 새로운 기능이 나타날 수 있으며, 이러한 기능은 이전에 연구된 규모를 넘어 계속 나타날 수 있음을 시사합니다.</p><blockquote><p><img loading="lazy" alt="discontinuous-improvements" src="/en/assets/images/discontinuous-improvements-309574e5d88d2788b5667c9f1c9bb6ed.gif" width="1600" height="583" class="img_ev3q"><br>
<!-- -->대규모 모델에선 새로운 기능이 나타날 수 있습니다 (출처: 구글 블로그)</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="다국어-이해">다국어 이해<a href="#다국어-이해" class="hash-link" aria-label="Direct link to 다국어 이해" title="Direct link to 다국어 이해">​</a></h3><p><em>Multilingual understanding</em></p><p>대규모 언어 모델에 대한 이전 작업은 다국어 영역에서 제한된 평가를 수행했습니다.
이 논문에서, 우리는 다양한 언어로 기계 번역(섹션 <a href="#65-translation">6.5</a>), 요약(섹션 <a href="#66-multilingual-natural-language-generation">6.6</a>) 및 질문 답변(섹션 <a href="#67-multilingual-question-answering">6.7</a>)을 포함한 다국어 벤치마크에 보다 철저한 평가를 수행합니다.
학습 말뭉치(corpus)에서 비영어 데이터(≈ 22%)의 상대적으로 적은 비율에도 불구하고,
540B 모델의 few-shot 결과는 비영어 요약 작업에서 이전의 finetuned sota와 격차를 해소하고 번역 작업에서 이전의 sota를 능가할 수 있었습니다.
영어 및 다국어 작업에 대한 다국어 데이터의 비율을 증가시키는 영향을 이해하기 위해서 further work이 필요합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="편견과-독성">편견과 독성<a href="#편견과-독성" class="hash-link" aria-label="Direct link to 편견과 독성" title="Direct link to 편견과 독성">​</a></h3><p><em>Bias and toxicity</em></p><p>또한 편견과 독성에 대한 모델 성능을 평가하여 몇 가지 통찰력을 얻었습니다(섹션 <a href="#10-%EB%8C%80%ED%91%9C%EC%A0%81%EC%9D%B8-%ED%8E%B8%EA%B2%AC-%EB%B6%84%EC%84%9D">10</a>).
첫째, 성별과 직업 편견으로 Winogender conference task가 정확성이 모델 규모에 따라 향상된다는 것을 발견했으며,
PaLM 540B는 1-shot 또는 few-shot 설정에서 새로운 sota를 달성합니다.
둘째, 인종/종교/성별이 prompt continuation으로 수행된 동시 분석은 모델이 무슬림을 테러리즘, 극단주의 및 폭력과 연관시키는 고정관념을 잘못 주장하는 가능성을 보입니다.
이 행동은 모델 규모(scale) 전반에 걸쳐 일관성이 있었습니다.
마지막으로, prompt continuation에 대한 독성 분석은 8B 모델에 비해 62B 및 540B 모델의 전반적인 독성 수준이 약간 더 높다는 것을 보여줍니다.
그러나, model-generated continuation의 독성은 프롬프트 텍스트의 독성과 높은 상관 관계가 있는 반면,
human-generated continuations은 강한 독성 상관 관계가 없습니다.
이것은 모델이 인간이 생성한 텍스트보다 프롬프트 스타일에 더 크게 영향을 받는다는 것을 시사합니다.
향후 작업에서, 이러한 벤치마크를 영어가 아닌 언어로 확대하고 잠재적인 위험을 더 철저하게 설명할 계획입니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="2-모델-아키텍처">2. 모델 아키텍처<a href="#2-모델-아키텍처" class="hash-link" aria-label="Direct link to 2. 모델 아키텍처" title="Direct link to 2. 모델 아키텍처">​</a></h2><p>PaLM은 표준 <a href="/en/posts/attention-is-all-you-need">transformer</a> 아키텍처에서 디코더만 있는 설정을 사용했고
(즉, 각 timestep은 자신과 과거 timesteps만 참고 가능합니다) 다음 변경을 가했습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="swiglu-activation">SwiGLU Activation<a href="#swiglu-activation" class="hash-link" aria-label="Direct link to SwiGLU Activation" title="Direct link to SwiGLU Activation">​</a></h3><p>표준 ReLU, GeLU 또는 <a href="#glu-variants-improve-transformer">Swish activations</a>에 비해 성능이 크게 향상되는 것으로 나타난,
MLP intermediate activations에 SwiGLU activations (Swish(xW) · xV)를 사용합니다.
MLP에서 두 개가 아닌 세 개의 행렬 곱셈이 필요하지만,
<a href="#glu-variants-improve-transformer">Swish activations</a>에서 컴퓨팅 등가 실험
(즉, 표준 ReLU variant가 비례적으로 더 큰 차원을 가진 경우)에서 성능 향상을 입증합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="parallel-layers">Parallel Layers<a href="#parallel-layers" class="hash-link" aria-label="Direct link to Parallel Layers" title="Direct link to Parallel Layers">​</a></h3><p>각 트랜스포머 블록에 표준 직렬화된 식이 아닌 병렬(parallel) 식을 사용합니다.
<a href="#a-6-billion-parameter-autoregressive-language-model">A 6 Billion Parameter Autoregressive Language Model</a>
표준 식이 다음과 같이 쓴다면</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = x + MLP(LayerNorm(x+Attention(LayerNorm(x))))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))))</span></span></span></span></span></div><p>병렬 식은 다음과 같이 쓸 수 있습니다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></div><p>MLP와 Attention 입력 matrix 곱셈은 합쳐질 수 있기 때문에,
대규모 스케일에서 병렬식이 대략 15% 빠르게 동작합니다.
Ablation 실험은 8B 크기에서 작은 품질 저하를 보여주었지만 62B 척도에서는 품질 저하가 없었기 때문에,
우리는 병렬 layer의 효과가 540B 크기에서 품질 저하가없다고(neutral) 추정합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="multi-query-attention">Multi-Query Attention<a href="#multi-query-attention" class="hash-link" aria-label="Direct link to Multi-Query Attention" title="Direct link to Multi-Query Attention">​</a></h3><p>표준 transformer 식은 각 시간 단계의 입력 벡터가 텐서 모양 <!-- -->[k,h]<!-- -->의, 여기서 h는 attention head 크기입니다,
&quot;query&quot;, &quot;key&quot; 및 &quot;value&quot;가 선형으로 사영(projected)되는 k attention 헤드를 사용합니다.
여기서, key/value 사영 예측은 각 헤드에 대해 공유됩니다.
즉, &quot;key&quot;와 &quot;value&quot;는 <!-- -->[1, h]<!-- -->로 투영되지만, &quot;query&quot;는 여전히 <!-- -->[k, h]<!-- -->를 형성하기 위해 사영됩니다.
이는 모델 품질과 훈련 속도에 영향이 없지만(neutral),
autoregressive 디코딩 시간에 상당한 비용 절감을 가져온다는 것을 발견했습니다.
<!-- -->[One write-head is all you need. Shazeer, 2019]<!-- -->
이것은 표준 multi-head attention이 autoregressive 디코딩 중에 가속기 하드웨어의 효율성이 낮기 때문입니다.
왜냐하면 key/value 텐서는 예제(examples) 간에 공유되지 않고, 한 번에 하나의 토큰만 디코딩되기 때문입니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rope-embeddings">RoPE Embeddings<a href="#rope-embeddings" class="hash-link" aria-label="Direct link to RoPE Embeddings" title="Direct link to RoPE Embeddings">​</a></h3><p>절대적이거나 상대적인 position embeddings이 아닌 RoPE 임베딩을 사용합니다.
RoPE 임베딩이 더 긴 시퀀스 길이에서 더 좋은 효과를 보였기 때문입니다.
<a href="#roformer-enhanced-transformer-with-rotary-position-embedding">Roformer: Enhanced transformer with rotary position embedding</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="shared-input-output-embeddings">Shared Input-Output Embeddings<a href="#shared-input-output-embeddings" class="hash-link" aria-label="Direct link to Shared Input-Output Embeddings" title="Direct link to Shared Input-Output Embeddings">​</a></h3><p>과거 작업에서 자주 (but not universally) 수행되는 입력 및 출력 임베딩 매트릭스를 공유합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="no-biases">No Biases<a href="#no-biases" class="hash-link" aria-label="Direct link to No Biases" title="Direct link to No Biases">​</a></h3><p>밀도 높은 커널(dense kernel)이나 layer norms에는 bias가 사용되지 않았습니다.
이는 대형 모델의 훈련 안정성을 증가시키는 것을 발견합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="vocabulary">Vocabulary<a href="#vocabulary" class="hash-link" aria-label="Direct link to Vocabulary" title="Direct link to Vocabulary">​</a></h3><p>우리는 과도한(excess) 토큰화 없이 훈련 말뭉치(corpus)에서 많은 수의 언어를 지원하기 위해 선택된 256k 토큰으로 <a href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing">SentencePiece</a> 어휘(vocabulary)를 사용합니다.
어휘(vocavulary)는 훈련 데이터에서 생성되었고, 훈련 효율성을 향상시킨다는 것을 발견했습니다.
어휘는 완전히 무손실(lossless)이며 되돌릴 수 있습니다(reversible).
이것은 공백이 어휘(특히 코드에 중요한)에 완전히 보존되고 어휘외(out-of-vocabulary) 유니코드 문자는 각 바이트에 대한 어휘 토큰과 함께 UTF-8 바이트로 나뉩니다.
숫자는 항상 개별 숫자 토큰으로 나뉩니다 (예: &quot;123.5 → 1 2 3 . 5&quot;).</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="21-모델-스케일-하이퍼파라미터">2.1 모델 스케일 하이퍼파라미터<a href="#21-모델-스케일-하이퍼파라미터" class="hash-link" aria-label="Direct link to 2.1 모델 스케일 하이퍼파라미터" title="Direct link to 2.1 모델 스케일 하이퍼파라미터">​</a></h3><p><em>Model Scale Hyperparameters</em></p><p>여기에서는 540B, 62B, 8B 세 모델 크기를 비교합니다.
FLOPs는 대략 파라미터 수와 비슷한데, 이 모델이 표준 dense transformer이기 때문입니다.
이 모델은 표1의 하이퍼파리미터로 구성되었습니다.
같은 데이터와 어휘를 사용해 세 모델은 독립적으로 학습되었습니다.
학습에 관한 내용은 섹션 <a href="#3-%ED%95%99%EC%8A%B5-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B">3</a>과 <a href="#5-%ED%95%99%EC%8A%B5-%EC%85%8B%EC%97%85">5</a>에 더 자세히 설명되어 있습니다.</p><blockquote><table><thead><tr><th>Model</th><th>Layers</th><th># of Heads</th><th>d_model</th><th># of Parameters(in billions)</th><th>Batch Size</th></tr></thead><tbody><tr><td>PaLM 8B</td><td>32</td><td>16</td><td>4096</td><td>8.63</td><td>256 -&gt; 512</td></tr><tr><td>PaLM 62B</td><td>64</td><td>32</td><td>8192</td><td>62.50</td><td>512 -&gt; 1024</td></tr><tr><td>PaLM 540B</td><td>118</td><td>48</td><td>18432</td><td>540.35</td><td>512 -&gt; 1024 -&gt; 2048</td></tr></tbody></table><p><em>표1</em> 모델 아키텍처 상세. feed-forward 크기는 항상 4*d_model이고 attention head 크기는 항상 256입니다.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="22-모델-카드">2.2 모델 카드<a href="#22-모델-카드" class="hash-link" aria-label="Direct link to 2.2 모델 카드" title="Direct link to 2.2 모델 카드">​</a></h3><p>PaLM에 대한 <a href="#model-cards-for-model-reporting">Model Card</a>는 부록 E에 있습니다.
이는 모델 아키텍처, 학습 설정, 학습 데이터, 확장된 사용을 위한 고수준의 요약입니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-학습-데이터셋">3 학습 데이터셋<a href="#3-학습-데이터셋" class="hash-link" aria-label="Direct link to 3 학습 데이터셋" title="Direct link to 3 학습 데이터셋">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="4-학습-인프라">4 학습 인프라<a href="#4-학습-인프라" class="hash-link" aria-label="Direct link to 4 학습 인프라" title="Direct link to 4 학습 인프라">​</a></h2><p>Pathways
Client-Server two-way data parallelism</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="41-학습-효율">4.1 학습 효율<a href="#41-학습-효율" class="hash-link" aria-label="Direct link to 4.1 학습 효율" title="Direct link to 4.1 학습 효율">​</a></h3><p>Hardware FLOPs Utilization (HFU).
주어진 장치에서 관측되는 이론상 최대 FLOPs에 대한 비율 추측을 나타냅니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="5-학습-셋업">5. 학습 셋업<a href="#5-학습-셋업" class="hash-link" aria-label="Direct link to 5. 학습 셋업" title="Direct link to 5. 학습 셋업">​</a></h2><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="51-학습-불안정">5.1 학습 불안정<a href="#51-학습-불안정" class="hash-link" aria-label="Direct link to 5.1 학습 불안정" title="Direct link to 5.1 학습 불안정">​</a></h3><h2 class="anchor anchorWithStickyNavbar_LWe7" id="6-평가">6 평가<a href="#6-평가" class="hash-link" aria-label="Direct link to 6 평가" title="Direct link to 6 평가">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="61-english-nlp-tasks">6.1 English NLP tasks<a href="#61-english-nlp-tasks" class="hash-link" aria-label="Direct link to 6.1 English NLP tasks" title="Direct link to 6.1 English NLP tasks">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="62-big-bench">6.2 BIG-bench<a href="#62-big-bench" class="hash-link" aria-label="Direct link to 6.2 BIG-bench" title="Direct link to 6.2 BIG-bench">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="63-reasoning">6.3 Reasoning<a href="#63-reasoning" class="hash-link" aria-label="Direct link to 6.3 Reasoning" title="Direct link to 6.3 Reasoning">​</a></h3><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="64-code-tasks">6.4 Code Tasks<a href="#64-code-tasks" class="hash-link" aria-label="Direct link to 6.4 Code Tasks" title="Direct link to 6.4 Code Tasks">​</a></h3><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="65-translation">6.5 Translation<a href="#65-translation" class="hash-link" aria-label="Direct link to 6.5 Translation" title="Direct link to 6.5 Translation">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="66-multilingual-natural-language-generation">6.6 Multilingual Natural Language Generation<a href="#66-multilingual-natural-language-generation" class="hash-link" aria-label="Direct link to 6.6 Multilingual Natural Language Generation" title="Direct link to 6.6 Multilingual Natural Language Generation">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="67-multilingual-question-answering">6.7 Multilingual Question Answering<a href="#67-multilingual-question-answering" class="hash-link" aria-label="Direct link to 6.7 Multilingual Question Answering" title="Direct link to 6.7 Multilingual Question Answering">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="68-analysis">6.8 Analysis<a href="#68-analysis" class="hash-link" aria-label="Direct link to 6.8 Analysis" title="Direct link to 6.8 Analysis">​</a></h3><h2 class="anchor anchorWithStickyNavbar_LWe7" id="7-암기memorization">7 암기(Memorization)<a href="#7-암기memorization" class="hash-link" aria-label="Direct link to 7 암기(Memorization)" title="Direct link to 7 암기(Memorization)">​</a></h2><p>신경망이 학습 데이터를 암기할 가능성이 있다는건 널리 알려진 사실입니다. 실은, overfitting에 대한 정의이기도 합니다.
일반적으로, 이러한 유형의 암기는 모델이 작은 훈련 세트를 많이 통과할 때 발생합니다.
하지만, PaLM은 780억개의 토큰 뭉치(corpus)를 통해 한번의 패스로 훈련합니다.
반면에, PaLM은 굉장히 큰 용량을 지니고 있기 때문에 한번의 패스만으로도 훈련 데이터의 상당 부분을 암기할 수 있다는건 그럴듯합니다.
또한 웹 파생 말뭉치(corpora)의 거의 복제된 텍스트는 몇개의 구문(작은 변화 포함)이 학습 중간에 많이 보입니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="8-데이터셋-오염">8 데이터셋 오염<a href="#8-데이터셋-오염" class="hash-link" aria-label="Direct link to 8 데이터셋 오염" title="Direct link to 8 데이터셋 오염">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="9-설명-살펴보기exploring-explanations">9 설명 살펴보기(Exploring Explanations)<a href="#9-설명-살펴보기exploring-explanations" class="hash-link" aria-label="Direct link to 9 설명 살펴보기(Exploring Explanations)" title="Direct link to 9 설명 살펴보기(Exploring Explanations)">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="10-대표적인-편견-분석">10 대표적인 편견 분석<a href="#10-대표적인-편견-분석" class="hash-link" aria-label="Direct link to 10 대표적인 편견 분석" title="Direct link to 10 대표적인 편견 분석">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-윤리적인-고려사항">11 윤리적인 고려사항<a href="#11-윤리적인-고려사항" class="hash-link" aria-label="Direct link to 11 윤리적인 고려사항" title="Direct link to 11 윤리적인 고려사항">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="12-관련-작업">12 관련 작업<a href="#12-관련-작업" class="hash-link" aria-label="Direct link to 12 관련 작업" title="Direct link to 12 관련 작업">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="13-스케일링에-열린-질문">13 스케일링에 열린 질문<a href="#13-스케일링에-열린-질문" class="hash-link" aria-label="Direct link to 13 스케일링에 열린 질문" title="Direct link to 13 스케일링에 열린 질문">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="14-결론">14 결론<a href="#14-결론" class="hash-link" aria-label="Direct link to 14 결론" title="Direct link to 14 결론">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="google-blog"><a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" target="_blank" rel="noopener noreferrer">Google Blog</a><a href="#google-blog" class="hash-link" aria-label="Direct link to google-blog" title="Direct link to google-blog">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pathways-asynchronous-distributed-dataflow-for-ml"><a href="https://arxiv.org/pdf/2203.12533.pdf" target="_blank" rel="noopener noreferrer">Pathways: Asynchronous distributed dataflow for ML</a><a href="#pathways-asynchronous-distributed-dataflow-for-ml" class="hash-link" aria-label="Direct link to pathways-asynchronous-distributed-dataflow-for-ml" title="Direct link to pathways-asynchronous-distributed-dataflow-for-ml">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="chain-of-thought-prompting-elicits-reasoning-in-large-language-models"><a href="https://arxiv.org/pdf/2201.11903.pdf" target="_blank" rel="noopener noreferrer">Chain of thought prompting elicits reasoning in large language models</a><a href="#chain-of-thought-prompting-elicits-reasoning-in-large-language-models" class="hash-link" aria-label="Direct link to chain-of-thought-prompting-elicits-reasoning-in-large-language-models" title="Direct link to chain-of-thought-prompting-elicits-reasoning-in-large-language-models">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="glu-variants-improve-transformer"><a href="https://arxiv.org/pdf/2002.05202.pdf" target="_blank" rel="noopener noreferrer">GLU variants improve transformer</a><a href="#glu-variants-improve-transformer" class="hash-link" aria-label="Direct link to glu-variants-improve-transformer" title="Direct link to glu-variants-improve-transformer">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-6-billion-parameter-autoregressive-language-model"><a href="https://github.com/kingoflolz/mesh-transformer-jax" target="_blank" rel="noopener noreferrer">A 6 Billion Parameter Autoregressive Language Model</a><a href="#a-6-billion-parameter-autoregressive-language-model" class="hash-link" aria-label="Direct link to a-6-billion-parameter-autoregressive-language-model" title="Direct link to a-6-billion-parameter-autoregressive-language-model">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="roformer-enhanced-transformer-with-rotary-position-embedding"><a href="https://arxiv.org/pdf/2104.09864.pdf" target="_blank" rel="noopener noreferrer">Roformer: Enhanced transformer with rotary position embedding</a><a href="#roformer-enhanced-transformer-with-rotary-position-embedding" class="hash-link" aria-label="Direct link to roformer-enhanced-transformer-with-rotary-position-embedding" title="Direct link to roformer-enhanced-transformer-with-rotary-position-embedding">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing"><a href="https://aclanthology.org/D18-2012.pdf" target="_blank" rel="noopener noreferrer">A simple and language independent subword tokenizer and detokenizer for neural text processing</a><a href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing" class="hash-link" aria-label="Direct link to a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing" title="Direct link to a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="model-cards-for-model-reporting"><a href="https://dl.acm.org/doi/10.1145/3287560.3287596" target="_blank" rel="noopener noreferrer">Model cards for model reporting</a><a href="#model-cards-for-model-reporting" class="hash-link" aria-label="Direct link to model-cards-for-model-reporting" title="Direct link to model-cards-for-model-reporting">​</a></h3><p>6-2</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models"><a href="https://github.com/google/BIG-bench/" target="_blank" rel="noopener noreferrer">Beyond the imitation game: Measuring and extrapolating the capabilities of language models</a><a href="#beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models" class="hash-link" aria-label="Direct link to beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models" title="Direct link to beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models">​</a></h3></div><a href="https://www.buymeacoffee.com/jwher"><img style="margin:20px 0" src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=jwher&amp;button_colour=40DCA5&amp;font_colour=ffffff&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=FFDD00"></a><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_Wr5y"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/en/posts/tags/ml">ml</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/en/posts/tags/paper">paper</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/jwher/jwher.github.io/posts/2023-02-11-palm-scaling-language-modeling-with-pathways/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer><div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/posts/the-docker-engine"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">도커 엔진</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/posts/cudabook-1"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">대규모 병렬프로세서 프로그래밍(소개)</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#요약" class="table-of-contents__link toc-highlight">요약</a></li><li><a href="#1-소개" class="table-of-contents__link toc-highlight">1. 소개</a><ul><li><a href="#효율적인-스케일링" class="table-of-contents__link toc-highlight">효율적인 스케일링</a></li><li><a href="#스케일링을-통한-지속적인-향상" class="table-of-contents__link toc-highlight">스케일링을 통한 지속적인 향상</a></li><li><a href="#획기적인-능력" class="table-of-contents__link toc-highlight">획기적인 능력</a></li><li><a href="#불연속적인-성능-개선" class="table-of-contents__link toc-highlight">불연속적인 성능 개선</a></li><li><a href="#다국어-이해" class="table-of-contents__link toc-highlight">다국어 이해</a></li><li><a href="#편견과-독성" class="table-of-contents__link toc-highlight">편견과 독성</a></li></ul></li><li><a href="#2-모델-아키텍처" class="table-of-contents__link toc-highlight">2. 모델 아키텍처</a><ul><li><a href="#swiglu-activation" class="table-of-contents__link toc-highlight">SwiGLU Activation</a></li><li><a href="#parallel-layers" class="table-of-contents__link toc-highlight">Parallel Layers</a></li><li><a href="#multi-query-attention" class="table-of-contents__link toc-highlight">Multi-Query Attention</a></li><li><a href="#rope-embeddings" class="table-of-contents__link toc-highlight">RoPE Embeddings</a></li><li><a href="#shared-input-output-embeddings" class="table-of-contents__link toc-highlight">Shared Input-Output Embeddings</a></li><li><a href="#no-biases" class="table-of-contents__link toc-highlight">No Biases</a></li><li><a href="#vocabulary" class="table-of-contents__link toc-highlight">Vocabulary</a></li><li><a href="#21-모델-스케일-하이퍼파라미터" class="table-of-contents__link toc-highlight">2.1 모델 스케일 하이퍼파라미터</a></li><li><a href="#22-모델-카드" class="table-of-contents__link toc-highlight">2.2 모델 카드</a></li></ul></li><li><a href="#3-학습-데이터셋" class="table-of-contents__link toc-highlight">3 학습 데이터셋</a></li><li><a href="#4-학습-인프라" class="table-of-contents__link toc-highlight">4 학습 인프라</a><ul><li><a href="#41-학습-효율" class="table-of-contents__link toc-highlight">4.1 학습 효율</a></li></ul></li><li><a href="#5-학습-셋업" class="table-of-contents__link toc-highlight">5. 학습 셋업</a><ul><li><a href="#51-학습-불안정" class="table-of-contents__link toc-highlight">5.1 학습 불안정</a></li></ul></li><li><a href="#6-평가" class="table-of-contents__link toc-highlight">6 평가</a><ul><li><a href="#61-english-nlp-tasks" class="table-of-contents__link toc-highlight">6.1 English NLP tasks</a></li><li><a href="#62-big-bench" class="table-of-contents__link toc-highlight">6.2 BIG-bench</a></li><li><a href="#63-reasoning" class="table-of-contents__link toc-highlight">6.3 Reasoning</a></li><li><a href="#64-code-tasks" class="table-of-contents__link toc-highlight">6.4 Code Tasks</a></li><li><a href="#65-translation" class="table-of-contents__link toc-highlight">6.5 Translation</a></li><li><a href="#66-multilingual-natural-language-generation" class="table-of-contents__link toc-highlight">6.6 Multilingual Natural Language Generation</a></li><li><a href="#67-multilingual-question-answering" class="table-of-contents__link toc-highlight">6.7 Multilingual Question Answering</a></li><li><a href="#68-analysis" class="table-of-contents__link toc-highlight">6.8 Analysis</a></li></ul></li><li><a href="#7-암기memorization" class="table-of-contents__link toc-highlight">7 암기(Memorization)</a></li><li><a href="#8-데이터셋-오염" class="table-of-contents__link toc-highlight">8 데이터셋 오염</a></li><li><a href="#9-설명-살펴보기exploring-explanations" class="table-of-contents__link toc-highlight">9 설명 살펴보기(Exploring Explanations)</a></li><li><a href="#10-대표적인-편견-분석" class="table-of-contents__link toc-highlight">10 대표적인 편견 분석</a></li><li><a href="#11-윤리적인-고려사항" class="table-of-contents__link toc-highlight">11 윤리적인 고려사항</a></li><li><a href="#12-관련-작업" class="table-of-contents__link toc-highlight">12 관련 작업</a></li><li><a href="#13-스케일링에-열린-질문" class="table-of-contents__link toc-highlight">13 스케일링에 열린 질문</a></li><li><a href="#14-결론" class="table-of-contents__link toc-highlight">14 결론</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a><ul><li><a href="#google-blog" class="table-of-contents__link toc-highlight">Google Blog</a></li><li><a href="#pathways-asynchronous-distributed-dataflow-for-ml" class="table-of-contents__link toc-highlight">Pathways: Asynchronous distributed dataflow for ML</a></li><li><a href="#chain-of-thought-prompting-elicits-reasoning-in-large-language-models" class="table-of-contents__link toc-highlight">Chain of thought prompting elicits reasoning in large language models</a></li><li><a href="#glu-variants-improve-transformer" class="table-of-contents__link toc-highlight">GLU variants improve transformer</a></li><li><a href="#a-6-billion-parameter-autoregressive-language-model" class="table-of-contents__link toc-highlight">A 6 Billion Parameter Autoregressive Language Model</a></li><li><a href="#roformer-enhanced-transformer-with-rotary-position-embedding" class="table-of-contents__link toc-highlight">Roformer: Enhanced transformer with rotary position embedding</a></li><li><a href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing" class="table-of-contents__link toc-highlight">A simple and language independent subword tokenizer and detokenizer for neural text processing</a></li><li><a href="#model-cards-for-model-reporting" class="table-of-contents__link toc-highlight">Model cards for model reporting</a></li><li><a href="#beyond-the-imitation-game-measuring-and-extrapolating-the-capabilities-of-language-models" class="table-of-contents__link toc-highlight">Beyond the imitation game: Measuring and extrapolating the capabilities of language models</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">
        <div class="copyright">
          Copyright © 2023, made by JWHer.<span class="heart-icon"></span>
        </div>
        </div></div></div></footer></div>
<script src="/en/assets/js/runtime~main.050f862b.js"></script>
<script src="/en/assets/js/main.30d9351e.js"></script>
</body>
</html>
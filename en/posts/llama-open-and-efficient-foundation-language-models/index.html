<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">LLaMa 공개된 효율적인 언어 모델 | JWHer Tech Blog</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jwher.github.io/en/posts/llama-open-and-efficient-foundation-language-models"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="LLaMa 공개된 효율적인 언어 모델 | JWHer Tech Blog"><meta data-rh="true" name="description" content="LLaMA: Open and Efficient Foundation Language Models"><meta data-rh="true" property="og:description" content="LLaMA: Open and Efficient Foundation Language Models"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-05-30T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jwher"><meta data-rh="true" property="article:tag" content="ml,paper"><link data-rh="true" rel="icon" href="/en/img/logo.svg"><link data-rh="true" rel="canonical" href="https://jwher.github.io/en/posts/llama-open-and-efficient-foundation-language-models"><link data-rh="true" rel="alternate" href="https://jwher.github.io/en/posts/llama-open-and-efficient-foundation-language-models" hreflang="en"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/llama-open-and-efficient-foundation-language-models" hreflang="kr"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/llama-open-and-efficient-foundation-language-models" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/en/posts/rss.xml" title="JWHer Tech Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/en/posts/atom.xml" title="JWHer Tech Blog Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-XHBVCY40VB","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><link rel="stylesheet" href="/en/assets/css/styles.b876ed3d.css">
<link rel="preload" href="/en/assets/js/runtime~main.5a6ea9b6.js" as="script">
<link rel="preload" href="/en/assets/js/main.9c3efe19.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/en/"><div class="navbar__logo"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/en/img/logo.svg" alt="Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">JWHer Tech Blog</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/en/posts">Posts</a><a class="navbar__item navbar__link" href="/en/categories">Categories</a><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-github"></a><a href="https://www.linkedin.com/in/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-linkedin"></a><a href="https://www.instagram.com/jwher96" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-instagram"></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="posts__header_mMDK"><div class="filter_lpre"></div><h2 class="presentation__title">Posts</h2><h6 class="presentation__subtitle">Let thine heart retain my words: Keep my commandments, and live.</h6></div><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_re4s thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_pO2u margin-bottom--md">All posts</div><ul class="sidebarItemList_Yudw clean-list"><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/quantization">정확한 추론을 위한 양자화 기법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/the-case-for-the-reduced-instruction-set-computer">RISC 컴퓨터를 위한 사례</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/docker-networking">도커 네트워킹</a></li><li class="sidebarItem__DBe"><a aria-current="page" class="sidebarItemLink_mo7H sidebarItemLinkActive_I1ZP" href="/en/posts/llama-open-and-efficient-foundation-language-models">LLaMa 공개된 효율적인 언어 모델</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/euclidean-algorithm">유클리드 호제법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/chatgpt-duality-of-wealth-and-faith">ChatGPT: 부와 신앙의 이중성에 대해</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic2">코딩 기초 트레이닝2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cpp-coding-basic1">코딩 기초 트레이닝1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/discipleship-training-1">지도자의 부르심</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/the-docker-engine">도커 엔진</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/palm">Pathways를 이용한 언어모델 스케일링</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cudabook-1">대규모 병렬프로세서 프로그래밍(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/attention-is-all-you-need">Attention 다시보기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-4">7가지 동시성 모델(스레드와 락) 거인의 어깨 위에서</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-3">7가지 동시성 모델(스레드와 락) 고유 락 개선하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-2">7가지 동시성 모델(스레드와 락) 상호 배제와 메모리 모델</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/concurrency-models-1">7가지 동시성 모델(소개)</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/requirement-levels">요구사항에 사용하는 RFC 키워드</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/hidden-technical-debt">머신러닝에 숨은 기술 부채</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/api-design-for-long-jobs">오래걸리는 API 설계</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deep-learning-on-a-data-diet">학습에 중요한 데이터 찾기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ngrx">NGRX 반응형 웹을 위한 상태 관리</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/power-series">다양한 급수</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/build-opencv-with-java">Build OpenCV with Java</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/binomial-theorem">이항정리 - π값을 구하는 법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nvidia-gpu-architectures">Nvidia GPU 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/pytorch-in-m1">Pytorch in M1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/agile">Agile</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post-with-docusaurus">First post with docusaurus</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/ensemble-methods">Ensemble Methods</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uncertainty-estimation">Uncertainty Estimation</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/Intelligent-Computer-Vision-1">Intelligent Computer Vision 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-architecture">쿠버네티스 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-guide">Kubeflow Guide</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/sagemaker">Sagemaker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-tech-map">Blog Tech Map</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/cncf">Cncf</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/envoy">Envoy</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/istio">Istio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/dex">Dex</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-configmap">K8S Tip Configmap</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/golang-setup">Golang Setup</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/docker-shared-volume">Docker Shared Volume</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/variable-autoencoder">Variable Autoencoder</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-pv-terminating">K8S Tip Pv Terminating</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/free-wildcard-dns">Free Wildcard Dns</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-harbor">Install Harbor</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-helm">Install Helm</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/github-issue">Github Issue</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-rollback">K8S Tip Rollback</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-expose-service">K8S Tip Expose Service</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/uuid">Uuid</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/deploying-ml-model-on-kubernetes-nuclio">Deploying Ml Model On Kubernetes Nuclio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-2">Kubeflow Visualization 2</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/linux-disk-free">Linux Disk Free</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-docker">Welcome To Docker</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubeflow">Welcome To Kubeflow</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/minio">Minio</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/k8s-tip-nodeselector">K8S Tip Nodeselector</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/information-theory">Information Theory</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubeflow-visualization-1">Kubeflow Visualization 1</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/nuclio">Nuclio 개념과 아키텍처</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/kubernetes-usage">자주쓰는 쿠버네티스 명령어</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/blog-essay">나에게 필요한 연재에 대해</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/alphapose">Alphapose 논문 리뷰와 사용</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/update-blog">지킬 블로그 업데이트</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-tar.gz">타르(tar) 파일 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubeflow">쿠브플로우를 설치하는 다양한 방법</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/virtualbox-with-no-gui">GUI 없이 버추얼박스 사용하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-docker">나에게 필요한 도커 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/install-kubernetes">나에게 필요한 쿠버네티스 설치하기</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/welcome-to-kubernetes">쿠버네티스 기본 개념과 필요성</a></li><li class="sidebarItem__DBe"><a class="sidebarItemLink_mo7H" href="/en/posts/first-post">First Post with Jekyll</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="title_xvU1" itemprop="headline">LLaMa 공개된 효율적인 언어 모델</h1><div class="container_iJTo margin-vert--md"><time datetime="2023-05-30T00:00:00.000Z" itemprop="datePublished">May 30, 2023</time> · <!-- -->16 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_q4o9"><div class="avatar margin-bottom--sm"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/jwher.png" alt="Jeongwon Her"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeongwon Her</span></a></div><small class="avatar__subtitle" itemprop="description">MLOps Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="/en/posts/llama-open-and-efficient-foundation-language-models"><img loading="lazy" alt="llama" src="/en/assets/images/llama-fd6ebe179d7441254555c0a0614b3551.jpeg" width="1200" height="1198" class="img_ev3q"></a><br>
<em>최대한 번역된 단어를 통일하였으나 원문을 보는것을 권장합니다.</em>  </p><h1>1. 소개</h1><p>LLMs(Large Languages Models)는 많은 텍스트 말뭉치를 통해 학습함으로써 텍스트 명령 또는 few examples에서 새로운 능력을 보여주었습니다. (<a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank" rel="noopener noreferrer">Brown et al.</a>)</p><p>few-shot 요소들은 모델을 충분한 크기로 키우는데서 처음 나타났고(Kaplan et al., 2020), 모델을 더 키우는 연구선상을 만듭니다. (<a href="https://arxiv.org/pdf/2204.02311.pdf" target="_blank" rel="noopener noreferrer">Chowdhery et al.</a>, 2022; Rae et al., 2021)</p><p>이 노력은 더 많은 파라미터가 더 나은 성능을 이끌거라는 가정하에 있습니다. 그러나 <a href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank" rel="noopener noreferrer">Hoffman et al.</a> 의 최근 연구는 주어진 컴퓨팅 예산에 큰 모델이 최고의 성능에 도달하지 못했고, 더 많은 데이터로 학습한 작은 모델이란 것을 보여줍니다.</p><p>Hoffmann et al.의 스케일링 법칙의 목적은 특정 학습을 위한 컴퓨팅 예산을 위해 데이터셋과 모델 크기를 가장 잘 확장하는 방법을 결정하는 것입니다.</p><p>그러나 이 목적은 언어 모델을 스케일링 할 때 치명적으로 작용하는, 추론 예산을 고려하지 않습니다.</p><p>성능 타깃 레벨이 주어진다는 맥락에서, 선호되는 모델은 학습에 빠른 것이 아니라 추론에 빠른 것이고, 비록 특정한 성능에 도달하기 위해 large model을 학습하는게 더 저렴할 수 있으나, 작은 것이 궁극적으로는 추론할 때 저렴할 것입니다.</p><p>예를 들어, Hoffmann et al.이 10B 모델을 200B 토큰으로 학습시킬 것을 추천했으나, 우리는 7B 모델이 1T 토큰 이후에도 지속적으로 성능이 증가하는 것을 확인할 수 있었습니다.</p><p>이 작업이 집중하는 것은 일반적으로 사용된 토큰보다 더 많은 토큰으로 학습시키면서 다양한 추론 예산에서 최적의(best possible) 성능을 달성하는 언어 모델 시리즈 학습입니다.</p><p>결과로 나온 모델은, LLaMA로 불리며, 7B~65B 파라미터를 가지고 현존하는 LLM들과 경쟁적인 성능을 냅니다.</p><p>예를 들어 LLaMa-13B는, GPT-3보다 10배 작지만 대부분 벤치마크를 상회합니다.</p><p>이 모델이 단일 GPU에서 실행할 수 있기 때문에, LLMs에 접근과 연구의 민주화(democratize)에 도움이 될 것으로 믿습니다.</p><p>확장의 최고점인 65B 파라미터 모델은 Chinchilla 또는 PaLM-540B와 같은 최고 LLMs와 경쟁적입니다.</p><p>Chinchilla, PaLM, GPT-3과 달리, 공개적으로 사용된 데이터만 사용했고, 오픈 소스와 호환되는 반면, 대부분의 모델이 공개적으로 사용 가능하지 않거나 문서화되지 않은 데이터에 의존하고 있습니다.</p><p>여기엔 몇 예외가 있습니다, OPT(Zhang et al.), GPT-NeoX(Black et al.), BLOOM(Scaoet et all.) and GLM(Zeng et al.), 그러나 어떤 것도 PaLM-62B 또는 Chinchilla와 비교할 수 없습니다.</p><p>이 논문에 나머지는, transformer(Vaswani et al) 아키텍처에 수행한 변경과, 학습 메소드를 소개합니다. 다음에는 모델의 성능과 다른 LLMs를 표준 벤치마크를 사용해 비교합니다. 최종적으로, 믿을만한 AI 커뮤니티에서 나온 가장 최신의 벤치마크 일부를 사용해서 편견과 독성이 모델에 인코딩 되었는지 노출합니다.</p><h1>2. 접근법</h1><p>학습 접근법은 이전 작업 (Brown et al., 2020; Chowdhery et al., 2022), 과 Chinchilla scaling laws (Hoffmann et al., 2022) 에서 영감을 받았습니다. 큰 transformers를 큰 텍스트 데이터로 표준 optimizer를 사용해 학습했습니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-pre-training-data">2.1 Pre-training Data<a href="#21-pre-training-data" class="hash-link" aria-label="Direct link to 2.1 Pre-training Data" title="Direct link to 2.1 Pre-training Data">​</a></h2><p>학습에 사용된 데이터셋은 Table 1에 보고한 것과 같이 여러 소스의 혼합이고, 다양한 도메인을 다룹니다.</p><blockquote><p>Table 1: Pre-training data. 사전 학습에 사용된 데이터 혼합으로, 각 하위집합에 샘플링 비율, 하위 집합을 1.4T 토큰을 학습할 때 수행된 epochs 수, 디스크 크기를 나열합니다. 1T 토큰으로 사전 학습한 것도 동일한 샘플링 비율을 가집니다.</p><p><img loading="lazy" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cc03535f-860b-492e-918b-f972a1bfd2da/Untitled.png" alt="Untitled" class="img_ev3q"></p></blockquote><p>대부분 분야에서,  대중에 공개된 데이터와, 오픈 소스 작업 호환성을 위한 데이터만 사용한 제한과 함께, 다른 LLM을 학습시키는데 사용한 데이터 소스를 재활용했습니다.</p><p>이것은 이어지는 데이터 혼합과 학습 셋에서 나타나는 퍼센테이지를 도출합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="english-commoncrawl-67">English CommonCrawl <!-- -->[67%]<a href="#english-commoncrawl-67" class="hash-link" aria-label="Direct link to english-commoncrawl-67" title="Direct link to english-commoncrawl-67">​</a></h3><p>5개의 CCNet pipeline (Wenzek et al., 2020)으로 2017~2020년 사이의 CommonCrawl 덤프를 전처리했습니다. 이 작업은 행 수준에서 데이터 중복을 제거하고, fastText 선형 분류기로 비영어 페이지와 저품질의 콘텐츠를 n-gram 언어 모델을 사용해 제거하는 언어 인식을 수행합니다. 추가적으로, Wikipedia에 참조된 페이지 vs 임의로 샘플된 페이지와 참조로 분류되지 않은 버려진 페이지를 분류하기 위해 선형 모델을 학습시켰습니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="c4-15">C4 <!-- -->[15%]<a href="#c4-15" class="hash-link" aria-label="Direct link to c4-15" title="Direct link to c4-15">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="github-45">Github <!-- -->[4.5%]<a href="#github-45" class="hash-link" aria-label="Direct link to github-45" title="Direct link to github-45">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="wikipedia-45">Wikipedia <!-- -->[4.5%]<a href="#wikipedia-45" class="hash-link" aria-label="Direct link to wikipedia-45" title="Direct link to wikipedia-45">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="gutenberg-and-books3-45">Gutenberg and Books3 <!-- -->[4.5%]<a href="#gutenberg-and-books3-45" class="hash-link" aria-label="Direct link to gutenberg-and-books3-45" title="Direct link to gutenberg-and-books3-45">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="arxiv-25">ArXiv <!-- -->[2.5%]<a href="#arxiv-25" class="hash-link" aria-label="Direct link to arxiv-25" title="Direct link to arxiv-25">​</a></h3><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stack-exchange-2">Stack Exchange <!-- -->[2%]<a href="#stack-exchange-2" class="hash-link" aria-label="Direct link to stack-exchange-2" title="Direct link to stack-exchange-2">​</a></h3><p>종합해서, 전체 학습 데이터셋은 토큰화 이후 대략 1.4T 토큰을 가집니다. Wikipedia와 Books 도메인에서 대략 2 epochs를 수행한 것을 제외하고, 대부분의 학습 데이터에서, 각 토큰은 학습할때 한번만 사용됩니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-architecture">2.2 Architecture<a href="#22-architecture" class="hash-link" aria-label="Direct link to 2.2 Architecture" title="Direct link to 2.2 Architecture">​</a></h2><p>최근 LLM 작업을 따라 네트워크는 transformer architecture를 따릅니다. 후속에 제안되고 PaLM과 같은 다른 모델에서 사용된 다양한 개선사항을 활용합니다. 여기에서 원래 아키텍처와 주된 다른점을 다루고, 어디에서 변경에 대한 영감을 찾았는지 <!-- -->[괄호에]<!-- --> 표시합니다.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="pre-normalization-gpt3">Pre-normalization <!-- -->[GPT3]<a href="#pre-normalization-gpt3" class="hash-link" aria-label="Direct link to pre-normalization-gpt3" title="Direct link to pre-normalization-gpt3">​</a></h3><p>학습 안정성을 개선하기 위해 출력을 normalize 하는 대신, 각 transformer의 sub-layer의 입력을 normalize 했습니다. Zhang and Sennrich가 도입한 RMSNorm을 normalizing 함수로 사용했습니다.</p><ul><li>transformer input/output 설명</li><li>RMSNorm 설명</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="swiglu-activation-function-palm">SwiGLU activation function <!-- -->[PaLM]<a href="#swiglu-activation-function-palm" class="hash-link" aria-label="Direct link to swiglu-activation-function-palm" title="Direct link to swiglu-activation-function-palm">​</a></h3><p>ReLU의 비선형성을 Shazeer가 성능 향상을 위해 도입한 SwiGLU activation function으로 바꾸었습니다. 2/3 4d 차원을 PaLM의 4d 대신 사용했습니다.</p><ul><li>ReLU 설명</li><li>SwiGLU 설명</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="rotary-embeddings-gptneo">Rotary Embeddings <!-- -->[GPTNeo]<a href="#rotary-embeddings-gptneo" class="hash-link" aria-label="Direct link to rotary-embeddings-gptneo" title="Direct link to rotary-embeddings-gptneo">​</a></h3><p>absolute positional embedding을 제거하고 각 네트워크의 레이어에서 Su et al이 도입한 rotary positional embeddings(RoPE)를 사용했습니다.</p><ul><li>Absolute Positional Embedding 설명</li><li>RoPE 설명</li></ul><p><img loading="lazy" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/d6554f37-cf9f-4a1c-bef9-0a90ebac959b/Untitled.png" alt="Untitled" class="img_ev3q"></p><p>변경한 모델에 대한 자세한 하이퍼파라미터는 표2에 주어집니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-optimizer">2.3 Optimizer<a href="#23-optimizer" class="hash-link" aria-label="Direct link to 2.3 Optimizer" title="Direct link to 2.3 Optimizer">​</a></h2><p>β1 = 0.9, β2 = 0.95의 하이퍼파라미터로, AdamW optimizer (Loshchilov and Hutter, 2017)를 사용해 모델을 학습했습니다. 마지막 learning rate가 10%의 최고 learning rate를 가지게 되는 코사인 learning rate schedule을 사용했습니다. Weight decay를 0.1, Gradient clipping 1.0을 사용했습니다. 2000 예열 단계를 거치고, 모델 크기에 따라 learning rate와 배치 크기를 조절합니다.</p><blockquote><p>Table 2: 모델 크기, 구조, 하이퍼 파라미터의 최적화</p><p><img loading="lazy" src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/8994f833-e6f5-4163-864c-43b8922fef95/Untitled.png" alt="Untitled" class="img_ev3q"></p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-efficient-implementation">2.4 Efficient implementation<a href="#24-efficient-implementation" class="hash-link" aria-label="Direct link to 2.4 Efficient implementation" title="Direct link to 2.4 Efficient implementation">​</a></h2><p>모델의 학습 속도를 개선하기 위해 몇 최적화를 만들었습니다.</p><p>첫째,  실행시간 메모리 사용량을 줄이도록 효율적인 causal multi-head attention의 효율적인 구현을 사용했습니다. Xformers 라이브러리2에서 사용할 수 있는 이 구현은 Rabe and Staats에서 영감을 얻었으며 Dao et al.의 backward를 사용합니다. (2022). 이것은 attention 가중치를 저장하지 않고 언어 모델링 작업의 인과적 특성으로 인해 가려진 키/쿼리 점수를 넣지 않음으로 달성합니다.</p><p>학습 효율을 더 향상시키기 위해, 체크포인팅을 하는 backward pass동안 재계산되는 activations의 양을 줄였습니다. 더 정확하게, 선형 계층의 출력같은 계산비용이 비싼 activations를 절약했습니다. 이것은 PyTorch autograd에 의존하는 대신 transformer 레이어의 backward function을 수동으로 구현함으로써 달성합니다. 이 최적화에서 최대한의 장점을 얻으려면, Korthikani가 기술한 모델과 시퀀스 병렬에 사용되는 메모리를 줄여야 했습니다. 게다가, 우리는 또한 가능한 한 (all_reduce 작업으로 인해) 네트워크를 통한 GPU 간의 activation 계산과 통신을 오버랩합니다.</p><p>65B-파라미터 모델을 훈련할 때, 우리의 코드는 80GB의 RAM이 있는 2048 A100 GPU에서 약 380개의 토큰/초/GPU를 처리합니다. 이것은 1.4T 토큰이 포함된 데이터 세트에 대한 훈련이 약 21일이 걸린다는 것을 의미합니다.</p><h1>3. 주요 성과</h1><p>이전 작업 (Brown et al., 2020)을 이어서 zero-shot과 few-shot작업을 고려해 20개의 벤치마크 결과를 보고합니다.</p><ul><li>Zero-shot: task와 test example에 대한 텍스트 설명을 제공합니다. 모델은 open-ended generation 또는 제안된 답변의 순위를 매겨 제공합니다.</li><li>Few-shot: 1~64 사이의 몇개의 task 예제와 테스트 예제를 제공합니다. 모델은이 텍스트를 입력으로 받고 답변을 생성하고나 다른 옵션의 순위를 매깁니다.</li></ul><p>대중이 사용가능하지 않은 언어 모델 GPT-3, Gopher, Chinchilla 그리고 PaLM과, open-sourced OPT 모델 GPT-J, GPT-Neo와 같은 다른 재단의 모델과 LLaMA를 비교합니다. 섹션4에서 OPT-IML과 Flan-PaLM과 같은 명령어 튜닝된 모델과 간단한 비교를 합니다.</p><p>free-form 생성 작업과 객관식(multiple choice) 작업에 대해 LLaMA를 평가합니다. 객관식 작업에서, 목표는 제공된 맥락에 따라 주어진 일련의 작업 중에서 가장 적절한 완성을 선택하는 것입니다. 제공된 맥락을 감안할 때 가장 높은 가능성의 완성을 선택합니다. Gao et al.을 따르고 Brown et al.을 따르는 특정 데이터 세트(OpenBookQA, BoolQ)를 제외하고 완성에 필요한 문자 수에 의해 normalized된 가능성을 사용하고, ㅁ문맥으로 &quot;Answer:&quot; 주어진 완성에 가능성이 높은 normalized된 가능성에 따라 완성을 선택합니다.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-common-sense-reasoning">3.1 Common Sense Reasoning<a href="#31-common-sense-reasoning" class="hash-link" aria-label="Direct link to 3.1 Common Sense Reasoning" title="Direct link to 3.1 Common Sense Reasoning">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-closed-book-question-answering">3.2 Closed-book Question Answering<a href="#32-closed-book-question-answering" class="hash-link" aria-label="Direct link to 3.2 Closed-book Question Answering" title="Direct link to 3.2 Closed-book Question Answering">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-reading-comprehension">3.3 Reading Comprehension<a href="#33-reading-comprehension" class="hash-link" aria-label="Direct link to 3.3 Reading Comprehension" title="Direct link to 3.3 Reading Comprehension">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-mathematical-reasoning">3.4 Mathematical reasoning<a href="#34-mathematical-reasoning" class="hash-link" aria-label="Direct link to 3.4 Mathematical reasoning" title="Direct link to 3.4 Mathematical reasoning">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="35-code-generation">3.5 Code generation<a href="#35-code-generation" class="hash-link" aria-label="Direct link to 3.5 Code generation" title="Direct link to 3.5 Code generation">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="36-massive-multitask-language-understanding">3.6 Massive Multitask Language Understanding<a href="#36-massive-multitask-language-understanding" class="hash-link" aria-label="Direct link to 3.6 Massive Multitask Language Understanding" title="Direct link to 3.6 Massive Multitask Language Understanding">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="37-evolution-of-performance-during-training">3.7 Evolution of performance during training<a href="#37-evolution-of-performance-during-training" class="hash-link" aria-label="Direct link to 3.7 Evolution of performance during training" title="Direct link to 3.7 Evolution of performance during training">​</a></h2><h1>4. 명령어 파인튜닝</h1><h1>5. 편견, 독성과 잘못된 정보</h1><h1>6. 탄소발자국</h1><h1>7. 관련 연구</h1><h1>8. 결론</h1><p>이 논문에서, 자유롭게 공개된 언어 모델의 나열과, state-of-the-art 재단 모델의 경쟁을 보여줍니다. 가장 주목할것은, LLaMA-13B는 GPT-3보다 10x 작으며 능가하는 성능을 보였다는 것과, LLaMA-65B는 Chinchilla-70B와 PaLM-540B과 경쟁합니다.</p><p>이전 연구와는 다르게, 독점적인 데이터셋에 의존하지 않고, 대중에 공개된 데이터만 사용해 sota 성능을 달성하는게 가능하단 것을 보여줍니다. 이 모델을 연구 커뮤니티에 공개하는게 LLM 개발을 가속화하고, 강건함을 증가시키고 알려진 독성과 편견을 완화시키는데 기여하길 바랍니다. 추가적으로, Chung et al. (2022)의 지침에 따라 이 모델을 미세 조정하면 유망한 결과를 얻을 수 있다는것을 보았고, 후속 연구에서 더 자세히 조사할 예정입니다. 최종적으로, 스케일링에서 지속적인 향상을 볼 수 있는 한, 큰 말뭉치로 학습한 더 큰 모델을 향후에 공개할 예정입니다.</p></div><a href="https://www.buymeacoffee.com/jwher"><img style="margin:20px 0" src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=jwher&amp;button_colour=40DCA5&amp;font_colour=ffffff&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=FFDD00"></a><footer class="row docusaurus-mt-lg blogPostFooterDetailsFull_Wr5y"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/en/posts/tags/ml">ml</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/en/posts/tags/paper">paper</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/jwher/jwher.github.io/tree/main/posts/2023-05-30-llama-open-and-efficient-foundation-language-models/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer><div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--prev" href="/en/posts/docker-networking"><div class="pagination-nav__sublabel">Newer Post</div><div class="pagination-nav__label">도커 네트워킹</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/en/posts/euclidean-algorithm"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">유클리드 호제법</div></a></nav></main><div class="col col--2"><div class="tableOfContents_bqdL thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#21-pre-training-data" class="table-of-contents__link toc-highlight">2.1 Pre-training Data</a><ul><li><a href="#english-commoncrawl-67" class="table-of-contents__link toc-highlight">English CommonCrawl 67%</a></li><li><a href="#c4-15" class="table-of-contents__link toc-highlight">C4 15%</a></li><li><a href="#github-45" class="table-of-contents__link toc-highlight">Github 4.5%</a></li><li><a href="#wikipedia-45" class="table-of-contents__link toc-highlight">Wikipedia 4.5%</a></li><li><a href="#gutenberg-and-books3-45" class="table-of-contents__link toc-highlight">Gutenberg and Books3 4.5%</a></li><li><a href="#arxiv-25" class="table-of-contents__link toc-highlight">ArXiv 2.5%</a></li><li><a href="#stack-exchange-2" class="table-of-contents__link toc-highlight">Stack Exchange 2%</a></li></ul></li><li><a href="#22-architecture" class="table-of-contents__link toc-highlight">2.2 Architecture</a><ul><li><a href="#pre-normalization-gpt3" class="table-of-contents__link toc-highlight">Pre-normalization GPT3</a></li><li><a href="#swiglu-activation-function-palm" class="table-of-contents__link toc-highlight">SwiGLU activation function PaLM</a></li><li><a href="#rotary-embeddings-gptneo" class="table-of-contents__link toc-highlight">Rotary Embeddings GPTNeo</a></li></ul></li><li><a href="#23-optimizer" class="table-of-contents__link toc-highlight">2.3 Optimizer</a></li><li><a href="#24-efficient-implementation" class="table-of-contents__link toc-highlight">2.4 Efficient implementation</a></li><li><a href="#31-common-sense-reasoning" class="table-of-contents__link toc-highlight">3.1 Common Sense Reasoning</a></li><li><a href="#32-closed-book-question-answering" class="table-of-contents__link toc-highlight">3.2 Closed-book Question Answering</a></li><li><a href="#33-reading-comprehension" class="table-of-contents__link toc-highlight">3.3 Reading Comprehension</a></li><li><a href="#34-mathematical-reasoning" class="table-of-contents__link toc-highlight">3.4 Mathematical reasoning</a></li><li><a href="#35-code-generation" class="table-of-contents__link toc-highlight">3.5 Code generation</a></li><li><a href="#36-massive-multitask-language-understanding" class="table-of-contents__link toc-highlight">3.6 Massive Multitask Language Understanding</a></li><li><a href="#37-evolution-of-performance-during-training" class="table-of-contents__link toc-highlight">3.7 Evolution of performance during training</a></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">
        <div class="copyright">
          Copyright © 2025, made by JWHer.<span class="heart-icon"></span>
        </div>
        </div></div></div></footer></div>
<script src="/en/assets/js/runtime~main.5a6ea9b6.js"></script>
<script src="/en/assets/js/main.9c3efe19.js"></script>
</body>
</html>
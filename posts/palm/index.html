<!doctype html>
<html lang="kr" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.20">
<link rel="alternate" type="application/rss+xml" href="/posts/rss.xml" title="JWHer Tech Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/posts/atom.xml" title="JWHer Tech Blog Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-XHBVCY40VB","auto"),ga("set","anonymizeIp",!0),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css" integrity="sha384-odtC+0UGzzFL/6PNoE8rX/SPcQDXBJ+uRepguP4QkPCm2LBxH3FA3y+fKSiJ+AmM" crossorigin="anonymous"><title data-rh="true">Pathways를 이용한 언어모델 스케일링 | JWHer Tech Blog</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://jwher.github.io/posts/palm"><meta data-rh="true" name="docusaurus_locale" content="kr"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="kr"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Pathways를 이용한 언어모델 스케일링 | JWHer Tech Blog"><meta data-rh="true" name="description" content="PaLM: Scaling Language Modeling with Pathways"><meta data-rh="true" property="og:description" content="PaLM: Scaling Language Modeling with Pathways"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2023-02-11T00:00:00.000Z"><meta data-rh="true" property="article:author" content="https://github.com/jwher"><meta data-rh="true" property="article:tag" content="ml,paper"><link data-rh="true" rel="icon" href="/img/logo.svg"><link data-rh="true" rel="canonical" href="https://jwher.github.io/posts/palm"><link data-rh="true" rel="alternate" href="https://jwher.github.io/en/posts/palm" hreflang="en"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/palm" hreflang="kr"><link data-rh="true" rel="alternate" href="https://jwher.github.io/posts/palm" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.bcb72c7e.css">
<link rel="preload" href="/assets/js/runtime~main.4d8b0f3c.js" as="script">
<link rel="preload" href="/assets/js/main.ae8d0bdc.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title text--truncate">JWHer Tech Blog</b></a></div><div class="navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/posts">Posts</a><a class="navbar__item navbar__link" href="/categories">Categories</a><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-github"></a><a href="https://www.linkedin.com/in/jwher" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-linkedin"></a><a href="https://www.instagram.com/jwher96" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link icon-instagram"></a><div class="toggle_S7eR colorModeToggle_vKtC"><button class="clean-btn toggleButton_rCf9 toggleButtonDisabled_Pu9x" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_v35p"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_nQuB"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_dLyj"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="posts__header_mMDK"><div class="filter_lpre"></div><h2 class="presentation__title">Posts</h2><h6 class="presentation__subtitle">Let thine heart retain my words: Keep my commandments, and live.</h6></div><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_TMXw thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_V4zb margin-bottom--md">All posts</div><ul class="sidebarItemList_uHd5 clean-list"><li class="sidebarItem_spIe"><a aria-current="page" class="sidebarItemLink_eqrF sidebarItemLinkActive_XZSJ" href="/posts/palm">Pathways를 이용한 언어모델 스케일링</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/cudabook-1">대규모 병렬프로세서 프로그래밍(소개)</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/attention-is-all-you-need">Attention 다시보기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/concurrency-models-4">7가지 동시성 모델(스레드와 락) 거인의 어깨 위에서</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/concurrency-models-3">7가지 동시성 모델(스레드와 락) 고유 락 개선하기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/concurrency-models-2">7가지 동시성 모델(스레드와 락) 상호 배제와 메모리 모델</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/concurrency-models-1">7가지 동시성 모델(소개)</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/requirement-levels">요구사항에 사용하는 RFC 키워드</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/hidden-technical-debt">머신러닝에 숨은 기술 부채</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/api-design-for-long-jobs">오래걸리는 API 설계</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/deep-learning-on-a-data-diet">학습에 중요한 데이터 찾기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/ngrx">NGRX 반응형 웹을 위한 상태 관리</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/power-series">다양한 급수</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/build-opencv-with-java">Build OpenCV with Java</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/binomial-theorem">이항정리 - π값을 구하는 법</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/nvidia-gpu-architectures">Nvidia GPU 아키텍처</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/pytorch-in-m1">Pytorch in M1</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/agile">Agile</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/first-post-with-docusaurus">First post with docusaurus</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/ensemble-methods">Ensemble Methods</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/uncertainty-estimation">Uncertainty Estimation</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/Intelligent-Computer-Vision-1">Intelligent Computer Vision 1</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/kubernetes-architecture">쿠버네티스 아키텍처</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/kubeflow-guide">Kubeflow Guide</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/sagemaker">Sagemaker</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/blog-tech-map">Blog Tech Map</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/cncf">Cncf</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/envoy">Envoy</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/istio">Istio</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/dex">Dex</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/k8s-tip-configmap">K8S Tip Configmap</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/golang-setup">Golang Setup</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/docker-shared-volume">Docker Shared Volume</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/variable-autoencoder">Variable Autoencoder</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/k8s-tip-pv-terminating">K8S Tip Pv Terminating</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/free-wildcard-dns">Free Wildcard Dns</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-harbor">Install Harbor</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-helm">Install Helm</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/github-issue">Github Issue</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/k8s-tip-rollback">K8S Tip Rollback</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/k8s-tip-expose-service">K8S Tip Expose Service</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/uuid">Uuid</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/deploying-ml-model-on-kubernetes-nuclio">Deploying Ml Model On Kubernetes Nuclio</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/kubeflow-visualization-2">Kubeflow Visualization 2</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/linux-disk-free">Linux Disk Free</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/welcome-to-docker">Welcome To Docker</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/welcome-to-kubeflow">Welcome To Kubeflow</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/minio">Minio</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/k8s-tip-nodeselector">K8S Tip Nodeselector</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/information-theory">Information Theory</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/kubeflow-visualization-1">Kubeflow Visualization 1</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/nuclio">Nuclio 개념과 아키텍처</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/kubernetes-usage">자주쓰는 쿠버네티스 명령어</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/blog-essay">나에게 필요한 연재에 대해</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/alphapose">Alphapose 논문 리뷰와 사용</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/update-blog">지킬 블로그 업데이트</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-tar.gz">타르(tar) 파일 설치하기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-kubeflow">쿠브플로우를 설치하는 다양한 방법</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/virtualbox-with-no-gui">GUI 없이 버추얼박스 사용하기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-docker">나에게 필요한 도커 설치하기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/install-kubernetes">나에게 필요한 쿠버네티스 설치하기</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/welcome-to-kubernetes">쿠버네티스 기본 개념과 필요성</a></li><li class="sidebarItem_spIe"><a class="sidebarItemLink_eqrF" href="/posts/first-post">First Post with Jekyll</a></li></ul></nav></aside><main class="col col--7" itemscope="" itemtype="http://schema.org/Blog"><article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h1 class="blogPostTitle_uMeh" itemprop="headline">Pathways를 이용한 언어모델 스케일링</h1><div class="blogPostData_Vfxe margin-vert--md"><time datetime="2023-02-11T00:00:00.000Z" itemprop="datePublished">February 11, 2023</time> · <!-- -->12 min read</div><div class="margin-top--md margin-bottom--sm row"><div class="col col--6 authorCol_FlmR"><div class="avatar margin-bottom--sm"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" class="avatar__photo-link"><img class="avatar__photo" src="https://github.com/jwher.png" alt="Jeongwon Her"></a><div class="avatar__intro" itemprop="author" itemscope="" itemtype="https://schema.org/Person"><div class="avatar__name"><a href="https://github.com/jwher" target="_blank" rel="noopener noreferrer" itemprop="url"><span itemprop="name">Jeongwon Her</span></a></div><small class="avatar__subtitle" itemprop="description">MLOps Engineer</small></div></div></div></div></header><div id="post-content" class="markdown" itemprop="articleBody"><p><a href="/posts/palm"><img loading="lazy" alt="palm" src="/assets/images/palm-886a5652802d433a8b9ffcae67a3abff.jpeg" width="400" height="427" class="img_E7b_"></a><br>
<em>최대한 번역된 단어를 통일하였으나 원문을 보는것을 권장합니다.</em>  </p><h2 class="anchor anchorWithStickyNavbar_mojV" id="abstract">Abstract<a class="hash-link" href="#abstract" title="Direct link to heading">​</a></h2><p>대규모 언어 모델은 few-shot learning을 사용한 다양한 자연어 처리 분야에서 눈에 띄는 성과를 보이고 있습니다.
이는 모델을 task-specific하게 적응하기 위한 예제 수를 수를 크게 줄입니다.
더 나아가 few-shot learning에 대한 영향에 대한 이해를 높이기 위해,
540억개의 매개 변수, densely activated, <a href="/posts/attention-is-all-you-need">transformer</a> 언어 모델인 Pathways Language Model(PaLM)을 학습시켰습니다.</p><p>PaLM은 6144 TPU v4칩에서 Pathways를 사용해 학습했습니다. Pathways는 다양한 TPU Pods에서 고효율로 학습하길 가능하게 하는 ML system입니다.
수백개의 <strong>언어 이해와 생성</strong> 벤치마크에서 few-shot learning의 sota를 달성함을 보여줍니다.
많은 작업에서 PaLM은 540B는 고성능을 달성합니다. <strong>multi-step reasoning tasks</strong>에서 sota를 달성하고,
최근의 BIG-bench 벤치마크에서 평균 사람보다 나은 성능을 보여줍니다.
상당수의 BIG-bench tasks는 모댈 규모에 따라 불연속적인 성능 개선을 보여주었고,
이는 가장 큰 모델로 확장함에 따라 급격한 성능 증가를 보였음을 뜻합니다.
PaLM은 다양한 벤치마크에서 증명되는 강한 <strong>multilingual tasks</strong>와 <strong>source code generation</strong>을 갖춥니다.
편견(bias)과 독성(toxicity)에 대한 포괄적인 분석을 제공하고, 모델 규모와 관련하여 훈련 데이터 암기(memorization)을 연구합니다.
최종적으로 LLM에 대한 윤리적 고려 사항과 점진적인 완화 전략에 대해 논의합니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="1-소개">1. 소개<a class="hash-link" href="#1-소개" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="efficient-scaling">Efficient scaling<a class="hash-link" href="#efficient-scaling" title="Direct link to heading">​</a></h3><p>하나의 모델을 학습하는데 천에서 만개의 고효율 가속기를 사용할 수 있게 하는 새로운 ML system
<a href="#pathways-asynchronous-distributed-dataflow-for-ml">Pathways</a>를 첫번째 대규모 사용을 보입니다.
Patyways를 사용하면서 이전에는 도달할 수 없었던 효율성 수준에서 6144TPU v4 칩에 대한 540B 매개 변수 언어 모델을 훈련했습니다.
대부분 이전의 대형 언어 모델은 단일 TPU 시스템(GLaM)이나 파이프라인 병렬 처리(GPipe)를 사용해
GPU 클러스터(-)나 여러 TPU v3 파드(Gopher)를 확장해 최대 4096 TPU v3 chips을 확장했습니다.
섹션 4에서 우리는 모델 FLOPS를 활용해 PaLM 540B를 6144 칩 두개의 TPU v4 Pods에서
매우 높은 46.2%라는 모델 FLOPs utilization을 달성하였고(이론적인 최고 수치에 비교한 수치)
57.8%의 하드웨어 FLOPs utilization을 보였습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="continued-improvements-from-scaling">Continued improvements from scaling<a class="hash-link" href="#continued-improvements-from-scaling" title="Direct link to heading">​</a></h3><p>섹션 6에서 다양한 주요 벤치마크를 수행하고, 상당한 차이를 보였습니다.
이것은 대규모 LM의 스케일링 개선이 정체되거나 포화점에 도달하지 않았다는 것을 보여줍니다.
표 4에서 가장 널리 평가된 29개의 영어 이해 벤치마크에서 28개의 sota를 달성했음을 보여줍니다.
이전에 수행된 다음 항목과 비교했습니다(GLaM, GPT-3, Megaron-Turing NLG, Gopher, Chinchilla, LaMDA)</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="breakthrough-capabilities">Breakthrough capabilities<a class="hash-link" href="#breakthrough-capabilities" title="Direct link to heading">​</a></h3><ul><li>Discontinuous improvements</li><li>Multilingual understanding</li><li>Bias and toxicity</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="2-모델-아키텍처">2. 모델 아키텍처<a class="hash-link" href="#2-모델-아키텍처" title="Direct link to heading">​</a></h2><p>PaLM은 표준 <a href="/posts/attention-is-all-you-need">transformer</a> 아키텍처에서 디코더만 있는 설정을 사용했고
(즉, 각 timestep은 자신과 과거 timesteps만 참고 가능합니다) 다음 변경을 가했습니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="swiglu-activation">SwiGLU Activation<a class="hash-link" href="#swiglu-activation" title="Direct link to heading">​</a></h3><p>표준 ReLU, GeLU 또는 <a href="#glu-variants-improve-transformer">Swish activations</a>에 비해 성능이 크게 향상되는 것으로 나타난,
MLP intermediate activations에 SwiGLU activations (Swish(xW) · xV)를 사용합니다.
MLP에서 두 개가 아닌 세 개의 행렬 곱셈이 필요하지만,
<a href="#glu-variants-improve-transformer">Swish activations</a>에서 컴퓨팅 등가 실험
(즉, 표준 ReLU variant가 비례적으로 더 큰 차원을 가진 경우)에서 성능 향상을 입증합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="parallel-layers">Parallel Layers<a class="hash-link" href="#parallel-layers" title="Direct link to heading">​</a></h3><p>각 트랜스포머 블록에 표준 직렬화된 식이 아닌 병렬(parallel) 식을 사용합니다.
<a href="#a-6-billion-parameter-autoregressive-language-model">A 6 Billion Parameter Autoregressive Language Model</a>
표준 식이 다음과 같이 쓴다면</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = x + MLP(LayerNorm(x+Attention(LayerNorm(x))))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))))</span></span></span></span></span></div><p>병렬 식은 다음과 같이 쓸 수 있습니다.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><mo>+</mo><mi>M</mi><mi>L</mi><mi>P</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>+</mo><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>L</mi><mi>a</mi><mi>y</mi><mi>e</mi><mi>r</mi><mi>N</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = x + MLP(LayerNorm(x)) + Attention(LayerNorm(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">L</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span><span class="mspace" style="margin-right:0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right:0.02778em">yer</span><span class="mord mathnormal" style="margin-right:0.10903em">N</span><span class="mord mathnormal" style="margin-right:0.02778em">or</span><span class="mord mathnormal">m</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span></span></div><p>MLP와 Attention 입력 matrix 곱셈은 합쳐질 수 있기 때문에,
대규모 스케일에서 병렬식이 대략 15% 빠르게 동작합니다.
Ablation 실험은 8B 크기에서 작은 품질 저하를 보여주었지만 62B 척도에서는 품질 저하가 없었기 때문에,
우리는 병렬 layer의 효과가 540B 크기에서 품질 저하가없다고(neutral) 추정합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="multi-query-attention">Multi-Query Attention<a class="hash-link" href="#multi-query-attention" title="Direct link to heading">​</a></h3><p>표준 transformer 식은 각 시간 단계의 입력 벡터가 텐서 모양 <!-- -->[k,h]<!-- -->의, 여기서 h는 attention head 크기입니다,
&quot;query&quot;, &quot;key&quot; 및 &quot;value&quot;가 선형으로 사영(projected)되는 k attention 헤드를 사용합니다.
여기서, key/value 사영 예측은 각 헤드에 대해 공유됩니다.
즉, &quot;key&quot;와 &quot;value&quot;는 <!-- -->[1, h]<!-- -->로 투영되지만, &quot;query&quot;는 여전히 <!-- -->[k, h]<!-- -->를 형성하기 위해 사영됩니다.
이는 모델 품질과 훈련 속도에 영향이 없지만(neutral),
autoregressive 디코딩 시간에 상당한 비용 절감을 가져온다는 것을 발견했습니다.
<!-- -->[One write-head is all you need. Shazeer, 2019]<!-- -->
이것은 표준 multi-head attention이 autoregressive 디코딩 중에 가속기 하드웨어의 효율성이 낮기 때문입니다.
왜냐하면 key/value 텐서는 예제(examples) 간에 공유되지 않고, 한 번에 하나의 토큰만 디코딩되기 때문입니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="rope-embeddings">RoPE Embeddings<a class="hash-link" href="#rope-embeddings" title="Direct link to heading">​</a></h3><p>절대적이거나 상대적인 position embeddings이 아닌 RoPE 임베딩을 사용합니다.
RoPE 임베딩이 더 긴 시퀀스 길이에서 더 좋은 효과를 보였기 때문입니다.
<a href="#roformer-enhanced-transformer-with-rotary-position-embedding">Roformer: Enhanced transformer with rotary position embedding</a></p><h3 class="anchor anchorWithStickyNavbar_mojV" id="shared-input-output-embeddings">Shared Input-Output Embeddings<a class="hash-link" href="#shared-input-output-embeddings" title="Direct link to heading">​</a></h3><p>과거 작업에서 자주 (but not universally) 수행되는 입력 및 출력 임베딩 매트릭스를 공유합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="no-biases">No Biases<a class="hash-link" href="#no-biases" title="Direct link to heading">​</a></h3><p>밀도 높은 커널(dense kernel)이나 layer norms에는 bias가 사용되지 않았습니다.
이는 대형 모델의 훈련 안정성을 증가시키는 것을 발견합니다.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="vocabulary">Vocabulary<a class="hash-link" href="#vocabulary" title="Direct link to heading">​</a></h3><p>우리는 과도한(excess) 토큰화 없이 훈련 단어집(corpus)에서 많은 수의 언어를 지원하기 위해 선택된 256k 토큰으로 <a href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing">SentencePiece</a> 어휘(vocabulary)를 사용합니다.
어휘(vocavulary)는 훈련 데이터에서 생성되었고, 훈련 효율성을 향상시킨다는 것을 발견했습니다.
어휘는 완전히 무손실(lossless)이며 되돌릴 수 있습니다(reversible).
이것은 공백이 어휘(특히 코드에 중요한)에 완전히 보존되고 어휘외(out-of-vocabulary) 유니코드 문자는 각 바이트에 대한 어휘 토큰과 함께 UTF-8 바이트로 나뉩니다.
숫자는 항상 개별 숫자 토큰으로 나뉩니다 (예: &quot;123.5 → 1 2 3 . 5&quot;).</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="21-모델-스케일-하이퍼파라미터">2.1 모델 스케일 하이퍼파라미터<a class="hash-link" href="#21-모델-스케일-하이퍼파라미터" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="22-모델-카드">2.2 모델 카드<a class="hash-link" href="#22-모델-카드" title="Direct link to heading">​</a></h3><p>Mitchell et al. 2019</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="3-학습-데이터셋">3 학습 데이터셋<a class="hash-link" href="#3-학습-데이터셋" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="4-학습-인프라">4 학습 인프라<a class="hash-link" href="#4-학습-인프라" title="Direct link to heading">​</a></h2><p>Pathways
Client-Server two-way data parallelism</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="41-학습-효율">4.1 학습 효율<a class="hash-link" href="#41-학습-효율" title="Direct link to heading">​</a></h3><p>Hardware FLOPs Utilization (HFU).
주어진 장치에서 관측되는 이론상 최대 FLOPs에 대한 비율 추측을 나타냅니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="5-학습-셋업">5. 학습 셋업<a class="hash-link" href="#5-학습-셋업" title="Direct link to heading">​</a></h2><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="51-학습-불안정">5.1 학습 불안정<a class="hash-link" href="#51-학습-불안정" title="Direct link to heading">​</a></h3><h2 class="anchor anchorWithStickyNavbar_mojV" id="6-평가">6 평가<a class="hash-link" href="#6-평가" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="61-english-nlp-tasks">6.1 English NLP tasks<a class="hash-link" href="#61-english-nlp-tasks" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="62-big-bench">6.2 BIG-bench<a class="hash-link" href="#62-big-bench" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="63-reasoning">6.3 Reasoning<a class="hash-link" href="#63-reasoning" title="Direct link to heading">​</a></h3><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="64-code-tasks">6.4 Code Tasks<a class="hash-link" href="#64-code-tasks" title="Direct link to heading">​</a></h3><ul><li></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="65-translation">6.5 Translation<a class="hash-link" href="#65-translation" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="66-multilingual-natural-language-generation">6.6 Multilingual Natural Language Generation<a class="hash-link" href="#66-multilingual-natural-language-generation" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="67-multilingual-question-answering">6.7 Multilingual Question Answering<a class="hash-link" href="#67-multilingual-question-answering" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="68-analysis">6.8 Analysis<a class="hash-link" href="#68-analysis" title="Direct link to heading">​</a></h3><h2 class="anchor anchorWithStickyNavbar_mojV" id="7-암기memorization">7 암기(Memorization)<a class="hash-link" href="#7-암기memorization" title="Direct link to heading">​</a></h2><p>신경망이 학습 데이터를 암기할 가능성이 있다는건 널리 알려진 사실입니다. 실은, overfitting에 대한 정의이기도 합니다.
일반적으로, 이러한 유형의 암기는 모델이 작은 훈련 세트를 많이 통과할 때 발생합니다.
하지만, PaLM은 780억개의 토큰 뭉치(corpus)를 통해 한번의 패스로 훈련합니다.
반면에, PaLM은 굉장히 큰 용량을 지니고 있기 때문에 한번의 패스만으로도 훈련 데이터의 상당 부분을 암기할 수 있다는건 그럴듯합니다.
또한 웹 파생 말뭉치(corpora)의 거의 복제된 텍스트는 몇개의 구문(작은 변화 포함)이 학습 중간에 많이 보입니다.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="8-데이터셋-오염">8 데이터셋 오염<a class="hash-link" href="#8-데이터셋-오염" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="9-설명-살펴보기exploring-explanations">9 설명 살펴보기(Exploring Explanations)<a class="hash-link" href="#9-설명-살펴보기exploring-explanations" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="10-대표적인-편견-분석">10 대표적인 편견 분석<a class="hash-link" href="#10-대표적인-편견-분석" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="11-윤리적인-고려사항">11 윤리적인 고려사항<a class="hash-link" href="#11-윤리적인-고려사항" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="12-관련-작업">12 관련 작업<a class="hash-link" href="#12-관련-작업" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="13-스케일링에-열린-질문">13 스케일링에 열린 질문<a class="hash-link" href="#13-스케일링에-열린-질문" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="14-결론">14 결론<a class="hash-link" href="#14-결론" title="Direct link to heading">​</a></h2><h2 class="anchor anchorWithStickyNavbar_mojV" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="pathways-asynchronous-distributed-dataflow-for-ml"><a href="https://arxiv.org/pdf/2203.12533.pdf" target="_blank" rel="noopener noreferrer">Pathways: Asynchronous distributed dataflow for ML</a><a class="hash-link" href="#pathways-asynchronous-distributed-dataflow-for-ml" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="glu-variants-improve-transformer"><a href="https://arxiv.org/pdf/2002.05202.pdf" target="_blank" rel="noopener noreferrer">GLU variants improve transformer</a><a class="hash-link" href="#glu-variants-improve-transformer" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="a-6-billion-parameter-autoregressive-language-model"><a href="https://github.com/kingoflolz/mesh-transformer-jax" target="_blank" rel="noopener noreferrer">A 6 Billion Parameter Autoregressive Language Model</a><a class="hash-link" href="#a-6-billion-parameter-autoregressive-language-model" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="roformer-enhanced-transformer-with-rotary-position-embedding"><a href="https://arxiv.org/pdf/2104.09864.pdf" target="_blank" rel="noopener noreferrer">Roformer: Enhanced transformer with rotary position embedding</a><a class="hash-link" href="#roformer-enhanced-transformer-with-rotary-position-embedding" title="Direct link to heading">​</a></h3><h3 class="anchor anchorWithStickyNavbar_mojV" id="a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing"><a href="https://aclanthology.org/D18-2012.pdf" target="_blank" rel="noopener noreferrer">A simple and language independent subword tokenizer and detokenizer for neural text processing</a><a class="hash-link" href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing" title="Direct link to heading">​</a></h3></div><a href="https://www.buymeacoffee.com/jwher"><img style="margin:20px 0" src="https://img.buymeacoffee.com/button-api/?text=Buy me a coffee&amp;emoji=&amp;slug=jwher&amp;button_colour=40DCA5&amp;font_colour=ffffff&amp;font_family=Cookie&amp;outline_colour=000000&amp;coffee_colour=FFDD00"></a><footer class="row docusaurus-mt-lg blogPostDetailsFull_enUA"><div class="col"><b>Tags:</b><ul class="tags_XVD_ padding--none margin-left--sm"><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/posts/tags/ml">ml</a></li><li class="tag_JSN8"><a class="tag_hD8n tagRegular_D6E_" href="/posts/tags/paper">paper</a></li></ul></div><div class="col margin-top--sm"><a href="https://github.com/jwher/jwher.github.io/posts/2023-02-11-palm-scaling-language-modeling-with-pathways/index.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div></footer><div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/posts/cudabook-1"><div class="pagination-nav__sublabel">Older Post</div><div class="pagination-nav__label">대규모 병렬프로세서 프로그래밍(소개)</div></a></nav></main><div class="col col--2"><div class="tableOfContents_cNA8 thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#abstract" class="table-of-contents__link toc-highlight">Abstract</a></li><li><a href="#1-소개" class="table-of-contents__link toc-highlight">1. 소개</a><ul><li><a href="#efficient-scaling" class="table-of-contents__link toc-highlight">Efficient scaling</a></li><li><a href="#continued-improvements-from-scaling" class="table-of-contents__link toc-highlight">Continued improvements from scaling</a></li><li><a href="#breakthrough-capabilities" class="table-of-contents__link toc-highlight">Breakthrough capabilities</a></li></ul></li><li><a href="#2-모델-아키텍처" class="table-of-contents__link toc-highlight">2. 모델 아키텍처</a><ul><li><a href="#swiglu-activation" class="table-of-contents__link toc-highlight">SwiGLU Activation</a></li><li><a href="#parallel-layers" class="table-of-contents__link toc-highlight">Parallel Layers</a></li><li><a href="#multi-query-attention" class="table-of-contents__link toc-highlight">Multi-Query Attention</a></li><li><a href="#rope-embeddings" class="table-of-contents__link toc-highlight">RoPE Embeddings</a></li><li><a href="#shared-input-output-embeddings" class="table-of-contents__link toc-highlight">Shared Input-Output Embeddings</a></li><li><a href="#no-biases" class="table-of-contents__link toc-highlight">No Biases</a></li><li><a href="#vocabulary" class="table-of-contents__link toc-highlight">Vocabulary</a></li><li><a href="#21-모델-스케일-하이퍼파라미터" class="table-of-contents__link toc-highlight">2.1 모델 스케일 하이퍼파라미터</a></li><li><a href="#22-모델-카드" class="table-of-contents__link toc-highlight">2.2 모델 카드</a></li></ul></li><li><a href="#3-학습-데이터셋" class="table-of-contents__link toc-highlight">3 학습 데이터셋</a></li><li><a href="#4-학습-인프라" class="table-of-contents__link toc-highlight">4 학습 인프라</a><ul><li><a href="#41-학습-효율" class="table-of-contents__link toc-highlight">4.1 학습 효율</a></li></ul></li><li><a href="#5-학습-셋업" class="table-of-contents__link toc-highlight">5. 학습 셋업</a><ul><li><a href="#51-학습-불안정" class="table-of-contents__link toc-highlight">5.1 학습 불안정</a></li></ul></li><li><a href="#6-평가" class="table-of-contents__link toc-highlight">6 평가</a><ul><li><a href="#61-english-nlp-tasks" class="table-of-contents__link toc-highlight">6.1 English NLP tasks</a></li><li><a href="#62-big-bench" class="table-of-contents__link toc-highlight">6.2 BIG-bench</a></li><li><a href="#63-reasoning" class="table-of-contents__link toc-highlight">6.3 Reasoning</a></li><li><a href="#64-code-tasks" class="table-of-contents__link toc-highlight">6.4 Code Tasks</a></li><li><a href="#65-translation" class="table-of-contents__link toc-highlight">6.5 Translation</a></li><li><a href="#66-multilingual-natural-language-generation" class="table-of-contents__link toc-highlight">6.6 Multilingual Natural Language Generation</a></li><li><a href="#67-multilingual-question-answering" class="table-of-contents__link toc-highlight">6.7 Multilingual Question Answering</a></li><li><a href="#68-analysis" class="table-of-contents__link toc-highlight">6.8 Analysis</a></li></ul></li><li><a href="#7-암기memorization" class="table-of-contents__link toc-highlight">7 암기(Memorization)</a></li><li><a href="#8-데이터셋-오염" class="table-of-contents__link toc-highlight">8 데이터셋 오염</a></li><li><a href="#9-설명-살펴보기exploring-explanations" class="table-of-contents__link toc-highlight">9 설명 살펴보기(Exploring Explanations)</a></li><li><a href="#10-대표적인-편견-분석" class="table-of-contents__link toc-highlight">10 대표적인 편견 분석</a></li><li><a href="#11-윤리적인-고려사항" class="table-of-contents__link toc-highlight">11 윤리적인 고려사항</a></li><li><a href="#12-관련-작업" class="table-of-contents__link toc-highlight">12 관련 작업</a></li><li><a href="#13-스케일링에-열린-질문" class="table-of-contents__link toc-highlight">13 스케일링에 열린 질문</a></li><li><a href="#14-결론" class="table-of-contents__link toc-highlight">14 결론</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a><ul><li><a href="#pathways-asynchronous-distributed-dataflow-for-ml" class="table-of-contents__link toc-highlight">Pathways: Asynchronous distributed dataflow for ML</a></li><li><a href="#glu-variants-improve-transformer" class="table-of-contents__link toc-highlight">GLU variants improve transformer</a></li><li><a href="#a-6-billion-parameter-autoregressive-language-model" class="table-of-contents__link toc-highlight">A 6 Billion Parameter Autoregressive Language Model</a></li><li><a href="#roformer-enhanced-transformer-with-rotary-position-embedding" class="table-of-contents__link toc-highlight">Roformer: Enhanced transformer with rotary position embedding</a></li><li><a href="#a-simple-and-language-independent-subword-tokenizer-and-detokenizer-for-neural-text-processing" class="table-of-contents__link toc-highlight">A simple and language independent subword tokenizer and detokenizer for neural text processing</a></li></ul></li></ul></div></div></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">
        <div class="copyright">
          Copyright © 2023, made by JWHer.<span class="heart-icon"></span>
        </div>
        </div></div></div></footer></div>
<script src="/assets/js/runtime~main.4d8b0f3c.js"></script>
<script src="/assets/js/main.ae8d0bdc.js"></script>
</body>
</html>
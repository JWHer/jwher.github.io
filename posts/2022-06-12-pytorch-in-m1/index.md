---
authors:
- jwher
description: M1 ë§¥ì—ì„œ pytorch GPU ê°€ì† ì‚¬ìš©í•´ë³´ê¸°
slug: pytorch-in-m1
tags:
- tech
title: Pytorch in M1
---

![m1](apple_new-m1-chip-graphic.jpg)
*Pytorch on Mac!*
<!--truncate-->

ì„¤ì¹˜ì™€ ì‚¬ìš©ë°©ë²•ë§Œ ë³´ì‹œê³  ì‹¶ì€ ë¶„ì€ [ê³µì‹ ë°œí‘œ](#ê³µì‹-ë°œí‘œ) ë‹¨ë½ì„ ë´ì£¼ì„¸ìš”!

## Pytorch

> ì´ ê¸€ì„ ì½ëŠ” ë¶„ì´ë¼ë©´, pytorchê°€ ë­”ì§€ ëª¨ë¥´ì‹œëŠ” ë¶„ì€ ì—†ì„ ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤ë§Œ ğŸ¤£

íŒŒì´í† ì¹˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ë¨¸ì‹  ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ
ì œí’ˆ ê°œë°œ ì—°êµ¬ë¥¼ ë¹ ë¥´ê²Œ í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤.

ë¨¸ì‹  ëŸ¬ë‹ íŠ¹ì„±ìƒ ë§ì€ í–‰ë ¬ ì—°ì‚°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤.
pythonì—ì„œëŠ” Numpyê°€ ëŒ€í‘œì ì¸ í–‰ë ¬ ê³„ì‚° ë¼ì´ë¸ŒëŸ¬ë¦¬ì£ .
pytorchì—ì„œëŠ” Numpyì˜ [ndarrays](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)ì™€ ë¹„ìŠ·í•˜ë©° GPU ê°€ì†ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” [tensor](https://pytorch.org/docs/stable/tensors.html)ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## GPU ê°€ì†

ê·¸ë ‡ë‹¤ë©´ ì™œ GPU ê°€ì†ì„ í•´ì•¼í• ê¹Œìš”?  

í•˜ë“œì›¨ì–´ ê°€ì†(Hardware Accelation)ì´ë€ íŠ¹ì •í•œ ì†Œí”„íŠ¸ì›¨ì–´ ì‘ì—…ì„ general-purpose CPU ë³´ë‹¤ ë¹ ë¥´ê²Œ ì‘ì—…í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ ì¡ŒìŠµë‹ˆë‹¤.

GPUê°€ ê·¸ë ‡ì£ , ê·¼ë° GPUì—ì„œ ê·¸ë˜í”½ ì‘ì—…ë§Œ í•˜ë“œì›¨ì–´ ê°€ì†ì„ í•  ìˆ˜ ìˆëŠ” ê²Œ ì•„ë‹™ë‹ˆë‹¤.
ì—­ìœ¼ë¡œ General-purpose on GPU, [GPGPU](https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units) ê¸°ìˆ ì´ ìˆìŠµë‹ˆë‹¤.

ëŒ€í‘œì ì¸ GPU hardware ì œì¡°ì‚¬ nvidiaëŠ” Compute Unified Device Architecture, [CUDA](https://developer.nvidia.com/cuda-toolkit)ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
ì´ ê°€ì†ì€ AIì™€ ë”¥ëŸ¬ë‹ì„ ë¹ ë¥´ê²Œ í•´ ì£¼ì—ˆìŠµë‹ˆë‹¤. GPU ê°€ê²©ì„ í­ë“±ì‹œí‚¬ ì •ë„ë¡œìš”!

## M1

ê·¸ë ‡ë‹¤ë©´ ë§¥ë¶ì€ Nvidia GPUë¥¼ ì“¸ê¹Œìš”?
ë‹µì€ ì´ì   **ì•„ë‹ˆìš”** ì…ë‹ˆë‹¤.

ì• í”Œì€ Macì„ Apple siliconìœ¼ë¡œ ì „í™˜í•œë‹¤ëŠ” ê³„íšì„ ë°œí‘œí•©ë‹ˆë‹¤.

[Apple announces Mac transition to Apple silicon](https://www.apple.com/newsroom/2020/06/apple-announces-mac-transition-to-apple-silicon/)  
[Apple unleashes M1](https://www.apple.com/newsroom/2020/11/apple-unleashes-m1/)

M1ì€ [ARMv8.5-A](https://en.wikipedia.org/wiki/ARM_architecture_family#ARMv8-A) [ëª…ë ¹ì–´ ì§‘í•©](https://github.com/llvm/llvm-project/blob/main/llvm/include/llvm/Support/AArch64TargetParser.def)ì„ ê°€ì§„ Advanced RISC Machine(ARM)ìœ¼ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
[RISC](https://en.wikipedia.org/wiki/Reduced_instruction_set_computer), Reduced Instruction Set ComputerëŠ” ì ì€ ëª…ë ¹ì–´ ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ì†ë„ë¥¼ ë†’ì¸ ë§ˆì´í¬ë¡œí”„ë¡œì„¸ì„œ ì…ë‹ˆë‹¤.
ê³ ì • ê¸¸ì´ì˜ ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ ë¹ ë¥´ê²Œ í•´ì„ë˜ê³  ëª…ë ¹ì–´ íŒŒì´í”„ë¼ì¸ ëŒ€ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.

ì´ëŠ” Intelê³¼ ê°™ì€ ì „í†µì ì¸ í•˜ë‚˜ì˜ íŠ¹ì •í•œ ì—°ì‚°ì„ ìœ„í•œ ë³µì¡í•œ ëª…ë ¹ì–´ë¥¼ ì œê³µí•˜ëŠ” [CISC](https://en.wikipedia.org/wiki/Complex_instruction_set_computer), Complex Instruction Set Computerì™€ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
CISCì— ë¹„í•´ RISCê°€ ê°–ëŠ” ëŒ€í‘œì ì¸ ì¥ì ìœ¼ë¡œ **ì „ë ¥ ì†Œëª¨ê°€ ì ë‹¤**ëŠ” ê²ƒì…ë‹ˆë‹¤.
ë”°ë¼ì„œ, ì €ì „ë ¥ìœ¼ë¡œ ì‘ë™í•´ì•¼ í•˜ëŠ” ë§ì€ ëª¨ë°”ì¼ ê¸°ê¸°ì™€ ì„ë² ë””ë“œ í”„ë¡œì„¸ì„œì—ì„œ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.

![apple_m1-chip-cpu-power-chart](apple_m1-chip-cpu-power-chart.jpg)

M1 ì¹©ì€ GPUì™€ Neural Engineì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ê·¸ë ‡ë‹¤ë©´, Macì—ì„œ GPU ê°€ì†ì„ ì‚¬ìš©í•´ ë¨¸ì‹ ëŸ¬ë‹ì„ í•˜ëŠ”ê±´ ë¶ˆê°€ëŠ¥ í• ê¹Œìš”?

## ê³µì‹ ë°œí‘œ

[Official Blog](https://pytorch.org/blog/introducing-accelerated-pytorch-training-on-mac/)
ì— ë”°ë¥´ë©´, pytorch ê°œë°œíŒ€ì€ ì• í”Œ Metal engineering íŒ€ê³¼ í•¨ê»˜ Mac GPU ê°€ì† íŒŒì´í† ì¹˜ í•™ìŠµì„ ë„ì…í–ˆìŠµë‹ˆë‹¤.
ì´ëŠ” appleì˜ [Metal Performance Shaders](https://developer.apple.com/videos/play/wwdc2021/10152/) ë¥¼ backendë¡œ í•´ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.

<!-- In collaboration with the Metal engineering team at Apple, we are excited to announce support for GPU-accelerated PyTorch training on Mac.
Accelerated GPU training is enabled using Appleâ€™s Metal Performance Shaders (MPS) as a backend for PyTorch. -->

ì´ëŠ” CPUë¡œ ì—°ì‚°ì„ í•  ë•Œ ë³´ë‹¤ ìµœì†Œ 5%ì—ì„œ 20% ì •ë„ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¸ë‹¤ê³  ë³´ê³ ë˜ì—ˆìŠµë‹ˆë‹¤.

![pytorch-m1](./pytorch-m1.gif)
* Testing conducted by Apple in April 2022 using production Mac Studio systems with Apple M1 Ultra, 20-core CPU, 64-core GPU 128GB of RAM, and 2TB SSD. Tested with macOS Monterey 12.3, prerelease PyTorch 1.12, ResNet50 (batch size=128), HuggingFace BERT (batch size=64), and VGG16 (batch size=64). Performance tests are conducted using specific computer systems and reflect the approximate performance of Mac Studio.

## ì„¤ì¹˜í•´ë³´ê¸°

í•˜ì§€ë§Œ ì§„ì§œ ê·¸ëŸ°ì§€ í™•ì¸í•´ ë´ì•¼ê² ì£ ?

ì„¤ì¹˜í•œ Library ì…ë‹ˆë‹¤. (Condaë‚˜ Venvë¥¼ ì‚¬ìš©í•˜ì‹œê¸¸ ì¶”ì²œí•©ë‹ˆë‹¤)
<details>
<summary>requirements.txt</summary>

```
certifi==2022.5.18.1
charset-normalizer==2.0.12
cycler==0.11.0
fonttools==4.33.3
idna==3.3
joblib==1.1.0
kiwisolver==1.4.2
matplotlib==3.5.2
numpy==1.23.0rc3
packaging==21.3
Pillow==9.1.1
pyparsing==3.0.9
python-dateutil==2.8.2
pytz==2022.1
requests==2.28.0
scikit-learn==1.1.1
scipy==1.8.1
six==1.16.0
sklearn==0.0
threadpoolctl==3.1.0
torch==1.13.0.dev20220611
torchaudio==0.14.0.dev20220603
torchvision==0.14.0.dev20220609
tqdm==4.64.0
typing_extensions==4.2.0
urllib3==1.26.9
```
</details>
<br/>

> **ì£¼ì˜**
>
> ì•ˆíƒ€ê¹ê²Œë„ 12.3+ ì´ìƒì˜ Mac OS ì—ì„œ ë™ì‘í•©ë‹ˆë‹¤. ì´ì „ ë²„ì „ì˜ OSì—ì„œ `torch.device("mps")` ë¥¼ ì‚¬ìš©í•˜ë©´ ë‹¤ìŒ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤.
> 
> ```
> The MPS backend is supported on MacOS 12.3+.Current OS version can be queried using `sw_vers`
> ```
<br/>

ì„¤ì¹˜ ì „ì— OS Versionì„ í™•ì¸í•©ì‹œë‹¤.
```
# Bad
jwher@jwher-MacBookPro ~ % sw_vers
ProductName:	macOS
ProductVersion:	12.2.1
BuildVersion:	21D62

# Good (12.3+)
jwher@jwher-MacBookPro ~ % sw_vers
ProductName:	macOS
ProductVersion:	12.4
BuildVersion:	21F79
```


## í…ŒìŠ¤íŠ¸

torchvisionì—ì„œ ì œê³µí•˜ëŠ” pre-trained weightë¡œ Mnistë¥¼ í•™ìŠµí•´ ë´…ì‹œë‹¤.

<details>
<summary>Full Script</summary>

```
# main threadì—ì„œë§Œ ì‹¤í–‰ê°€ëŠ¥í•œ í•¨ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤
if __name__ == "__main__":

    import platform, torch
    print(platform.platform())

    CPU= False
    device = "cpu" if CPU else torch.device("mps")
    print("Device is : {}".format(device))

    class CFG:
        lr = 0.001
        train_batch_size = 4
        total_epochs = 1
        num_classes = 10
        input_shape = (224,224)

    # Important to fix random seed
    torch.manual_seed(1)
    np.random.seed(1)

    from torch.utils.data import TensorDataset
    from torch.utils.data import DataLoader
    import torchvision
    from torchvision import datasets, transforms
    from torchvision.transforms import ToTensor

    image_path = "./data/"
    mnist_dataset = torchvision.datasets.MNIST(
        image_path, 'train', download=False,
        transform=transforms.Compose(
            [transforms.Resize(CFG.input_shape), transforms.Grayscale(3),ToTensor()]
        )
    )
    trainset_1 = torch.utils.data.Subset(mnist_dataset, list(range(1000)))
    mnist_loader  = DataLoader(trainset_1,batch_size=CFG.train_batch_size,shuffle=True,num_workers=4)
    x_batch, y_batch = (next(iter(mnist_loader)))

    import torchvision.models as models
    import torch.nn as nn
    import time, numpy as np
    from tqdm import tqdm

    class MODELS:
        vgg16_model = models.vgg16(pretrained=True)
        alexnet = models.alexnet(pretrained=True)
        resnet_18 = models.resnet18(pretrained=True)
        mobilenet_v2 = models.mobilenet_v2(pretrained=True)
        efficientnet_b0 = models.efficientnet_b0(pretrained=True)
        squeezenet = models.squeezenet1_0(pretrained=True)

        vgg16_model.classifier[6] = nn.Linear(vgg16_model.classifier[6].in_features,CFG.num_classes)
        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features,CFG.num_classes)
        resnet_18.fc = nn.Linear(resnet_18.fc.in_features,CFG.num_classes)
        mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features,CFG.num_classes)
        efficientnet_b0.classifier[1] = nn.Linear(efficientnet_b0.classifier[1].in_features,CFG.num_classes)
        squeezenet.classifier[1] = nn.Linear(squeezenet.classifier[1].in_channels,CFG.num_classes)

    def train(model_name,model,train_dl,n_epochs=CFG.total_epochs):
        '''
        call train_one_epoch:
        we will take average time taken to train per epoch for a max of 5 epochs
        '''
        loss_fn = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(),lr=CFG.lr)
        average_time = []
        for epoch in range(n_epochs):
            start_time = time.time()
            print(f"Epoch {epoch} -->")
            pbar = tqdm(enumerate(train_dl), total=len(train_dl), desc='Train : '+model_name)
            for step, (x_batch, y_batch) in pbar:   
                x_batch = x_batch.to(device)
                y_batch = y_batch.to(device)
                pred = model(x_batch)[:,0]
                loss = loss_fn(pred,y_batch.float())
                loss.backward()
                optimizer.step()
                optimizer.zero_grad()
            end_time = time.time() - start_time
            average_time.append(end_time)
        return np.mean(average_time)

    model_dict = {
        'vgg16_model' : MODELS.vgg16_model, 
        'alexnet' : MODELS.alexnet, 
        'resnet_18' : MODELS.resnet_18, 
        'mobilenet_v2' : MODELS.mobilenet_v2, 
        'efficientnet_b0' : MODELS.efficientnet_b0, 
        # 'squeezenet' : MODELS.squeezenet, 
    }

    time_calc = {}
    for model_name,model in model_dict.items():
        print("Model name is : {}".format(model_name))
        print("-----------------------------------------")
        model_epoch_avg_time = train(model_name,model.to(device),mnist_loader)
        time_calc[model_name] = model_epoch_avg_time

    print("Time result with device={}".format(device))
    for key, value in time_calc.items():
        print(f"{key}: {value}")
```

</details>

### CPU ê²°ê³¼

```
Time result with device=cpu
vgg16_model: 188.27877402305603
alexnet: 35.99792671203613
resnet_18: 42.05255889892578
mobilenet_v2: 223.11423015594482
efficientnet_b0: 256.45679998397827
```

### MPS ê²°ê³¼

```
Time result with device=mps
vgg16_model: 36.275963306427
alexnet: 10.713558197021484
resnet_18: 18.608237743377686
mobilenet_v2: 45.10530471801758
efficientnet_b0: 75.51856923103333
```

ì†ë„ ê°œì„ ë¥ ì„ ê³„ì‚°í•´ë³´ë©´,

| Model        | Speed Up |
|--------------|----------|
| vgg16        | 519.02%  |
| alexnet      | 336.00%  |
| resnet18     | 224.99%  |
| mobilenet v2 | 494.65%  |
| efficientnet | 339.59%  |
* 2022ë…„ 6ì›” Mac Book Pro M1 Max, 10-core CPU, 32-core GPU 64GB of RAM, 1TB SSDë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. macOS Monterey 12.4, prerelease PyTorch 1.13 í™˜ê²½, batch size=4ë¡œ í…ŒìŠ¤íŠ¸ í–ˆìŠµë‹ˆë‹¤.ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ëŠ” íŠ¹ì • ì»´í“¨í„° ì‹œìŠ¤í…œìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆê³  Mac Book Proì˜ ëŒ€ëµì ì¸ ì„±ëŠ¥ì„ ë°˜ì˜í•©ë‹ˆë‹¤.

**ë†€ëìŠµë‹ˆë‹¤!**

ê³µì‹ ë°œí‘œë³´ë‹¤ ì†ë„ ê°œì„ ì´ êµ‰ì¥íˆ ë†’ìŠµë‹ˆë‹¤. ì´ëŠ” ë°°ì¹˜ í¬ê¸°ê°€ ì‘ì•„ì„œ ìƒê¸´ ê²°ê³¼ ê°™ìŠµë‹ˆë‹¤. ì¶”í›„ì— 64 ë°°ì¹˜ë¡œ ë‹¤ì‹œ í…ŒìŠ¤íŠ¸í•˜ì—¬ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.

<details>
<summary>64 ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸ í•´ë´¤ëŠ”ë°...</summary>

```
# 64 batch CPU
Time result with device=cpu
vgg16_model: 217.3836727142334
alexnet: 18.045451164245605
resnet_18: 63.90257787704468
mobilenet_v2: 436.72195410728455
efficientnet_b0: 497.7381250858307

# 64 batch MPS
Time result with device=mps
vgg16_model: 35.657764196395874
alexnet: 4.972366094589233
resnet_18: 10.434533834457397
mobilenet_v2: 17.130388975143433
efficientnet_b0: 25.93315291404724
```

ì–´ë–¤ ëª¨ë¸ì—ì„  ì„±ëŠ¥ì´ ë” ì¢‹ìŠµë‹ˆë‹¤..?! ë‹¨ìˆœíˆ pytorch ê³µì‹ ë²¤ì¹˜ë§ˆí¬ë³´ë‹¤ ì„±ëŠ¥ì´ ë›°ì–´ë‚œê±¸ê¹Œìš”,
ì•„ë‹ˆë©´ ì‹¤í—˜ì´ ì˜ëª»ëœ ê±¸ê¹Œìš”. ì´ìœ ë¥¼ ì•„ì‹œëŠ” ë¶„ì´ ë‚˜íƒ€ë‚˜ì‹œê¸¸ ğŸ™ğŸ»

</details>

## References
[Pytorch training on m1 air gpu](https://abhishekbose550.medium.com/pytorch-training-on-m1-air-gpu-c534558acf1e)
